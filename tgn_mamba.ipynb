{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ilNrgYg4yWBP"
      },
      "id": "ilNrgYg4yWBP"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxry2znLyUIS",
        "outputId": "276f23ae-72a3-41b4-ab98-e7b6b4004ab2"
      },
      "id": "Jxry2znLyUIS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/mamba/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NjbbkflxrZf",
        "outputId": "d3db9f91-3df8-48a6-927e-ca3f8d4f3e4a"
      },
      "id": "2NjbbkflxrZf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/mamba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## !pip install  dgl -f https://data.dgl.ai/wheels/torch-2.1/cu118/repo.html\n",
        "## !pip install dgl-cu121\n",
        "## !pip install  dgl -f https://data.dgl.ai/wheels/torch-2.1/cu118/repo.html\n",
        "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.2/cu121/repo.html"
      ],
      "metadata": {
        "id": "zq8cg3uhBKVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe362b24-487c-46c4-bb80-f3d813bc96a8"
      },
      "id": "zq8cg3uhBKVd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/torch-2.2/cu121/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dgl) (2.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.2.1+cu121)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install causal-conv1d>=1.2.0\n",
        "!pip install mamba-ssm"
      ],
      "metadata": {
        "id": "3UkpLF4rxxPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f0e8a8-ea03-4c9b-f9de-38be35e20b7e"
      },
      "id": "3UkpLF4rxxPU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mamba-ssm in /usr/local/lib/python3.10/dist-packages (1.2.0.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.2.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.8.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.4.127)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##pip install  dgl -f https://data.dgl.ai/wheels/torch-2.1/cu121/repo.html\n",
        "# !pip install dgl -f https://data.dgl.ai/wheels/cu121/repo.html."
      ],
      "metadata": {
        "id": "Xyv4AQl20YF7"
      },
      "id": "Xyv4AQl20YF7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d34a0fa7-b951-4d31-8e5b-511fdbe70d7b",
      "metadata": {
        "id": "d34a0fa7-b951-4d31-8e5b-511fdbe70d7b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import ssl\n",
        "import time\n",
        "from six.moves import urllib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import dgl\n",
        "import copy\n",
        "import argparse\n",
        "import inspect\n",
        "from dgl.dataloading import Sampler\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7639cb1-8226-4a3d-80bf-747d7b602296",
      "metadata": {
        "id": "e7639cb1-8226-4a3d-80bf-747d7b602296"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--epochs\", type=int, default=50,help='epochs for training on entire dataset')\n",
        "parser.add_argument(\"--num_negative_samples\", type=int, default=1,\n",
        "                        help=\"number of negative samplers per positive samples\")\n",
        "parser.add_argument(\"--fast_mode\", action=\"store_true\", default=False,\n",
        "                        help=\"Fast Mode uses batch temporal sampling, history within same batch cannot be obtained\")\n",
        "parser.add_argument(\"--n_neighbors\", type=int, default=12,\n",
        "                        help=\"number of neighbors while doing embedding\")\n",
        "parser.add_argument(\"--batch_size\", type=int,\n",
        "                        default=1024, help=\"Size of each batch\")\n",
        "# parser.add_argument(\"--fast_mode\", action=\"store_true\", default=False,\n",
        "#                         help=\"Fast Mode uses batch temporal sampling, history within same batch cannot be obtained\")\n",
        "\n",
        "parser.add_argument(\"--embedding_dim\", type=int, default=100,\n",
        "                        help=\"Embedding dim for link prediction\")\n",
        "parser.add_argument(\"--memory_dim\", type=int, default=100,\n",
        "                        help=\"dimension of memory\")\n",
        "parser.add_argument(\"--temporal_dim\", type=int, default=100,\n",
        "                        help=\"Temporal dimension for time encoding\")\n",
        "parser.add_argument(\"--num_heads\", type=int, default=8,\n",
        "                        help=\"Number of heads for multihead attention mechanism\")\n",
        "parser.add_argument(\"--memory_updater\", type=str, default='ssm',\n",
        "                        help=\"Recurrent unit for memory update\")\n",
        "parser.add_argument(\"--k_hop\", type=int, default=1,\n",
        "                        help=\"sampling k-hop neighborhood\")\n",
        "parser.add_argument(\"--not_use_memory\", action=\"store_true\", default=False,\n",
        "                        help=\"Enable memory for TGN Model disable memory for TGN Model\")\n",
        "# parser.add_argument(\"--aggregator\", type=str, default='last',\n",
        "#                         help=\"Aggregation method for memory update\")\n",
        "args = parser.parse_args(args=[])\n",
        "# args = parser.parse_args()\n",
        "# args.epochs = 50\n",
        "data_file = \"wikipedia\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c203f34-8b4b-4601-9e62-4313b5b7fe80",
      "metadata": {
        "id": "8c203f34-8b4b-4601-9e62-4313b5b7fe80"
      },
      "outputs": [],
      "source": [
        "def preprocess(data_name):\n",
        "    u_list, i_list, ts_list, label_list = [], [], [], []\n",
        "    feat_l = []\n",
        "    idx_list = []\n",
        "\n",
        "    with open(data_name) as f:\n",
        "        s = next(f)\n",
        "        for idx, line in enumerate(f):\n",
        "            e = line.strip().split(',')\n",
        "            u = int(e[0])\n",
        "            i = int(e[1])\n",
        "\n",
        "            ts = float(e[2])\n",
        "            label = float(e[3])  # int(e[3])\n",
        "\n",
        "            feat = np.array([float(x) for x in e[4:]])\n",
        "\n",
        "            u_list.append(u)\n",
        "            i_list.append(i)\n",
        "            ts_list.append(ts)\n",
        "            label_list.append(label)\n",
        "            idx_list.append(idx)\n",
        "\n",
        "            feat_l.append(feat)\n",
        "    return pd.DataFrame({'u': u_list,\n",
        "                         'i': i_list,\n",
        "                         'ts': ts_list,\n",
        "                         'label': label_list,\n",
        "                         'idx': idx_list}), np.array(feat_l)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ef2821-e731-4eb8-ad94-48345f574479",
      "metadata": {
        "id": "c9ef2821-e731-4eb8-ad94-48345f574479"
      },
      "outputs": [],
      "source": [
        "def reindex(df, bipartite=True):\n",
        "    new_df = df.copy()\n",
        "    if bipartite:\n",
        "        assert (df.u.max() - df.u.min() + 1 == len(df.u.unique()))\n",
        "        assert (df.i.max() - df.i.min() + 1 == len(df.i.unique()))\n",
        "\n",
        "        upper_u = df.u.max() + 1\n",
        "        new_i = df.i + upper_u\n",
        "\n",
        "        new_df.i = new_i\n",
        "        new_df.u += 1\n",
        "        new_df.i += 1\n",
        "        new_df.idx += 1\n",
        "    else:\n",
        "        new_df.u += 1\n",
        "        new_df.i += 1\n",
        "        new_df.idx += 1\n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fedcdc54-9e60-4858-b8eb-91d0473bbcf3",
      "metadata": {
        "id": "fedcdc54-9e60-4858-b8eb-91d0473bbcf3"
      },
      "outputs": [],
      "source": [
        "def run(data_name, bipartite=True):\n",
        "    PATH = './data/{}.csv'.format(data_name)\n",
        "    OUT_DF = './data/ml_{}.csv'.format(data_name)\n",
        "    OUT_FEAT = './data/ml_{}.npy'.format(data_name)\n",
        "    OUT_NODE_FEAT = './data/ml_{}_node.npy'.format(data_name)\n",
        "\n",
        "    df, feat = preprocess(PATH)\n",
        "    new_df = reindex(df, bipartite)\n",
        "\n",
        "    empty = np.zeros(feat.shape[1])[np.newaxis, :]\n",
        "    feat = np.vstack([empty, feat])\n",
        "\n",
        "    max_idx = max(new_df.u.max(), new_df.i.max())\n",
        "    rand_feat = np.zeros((max_idx + 1, 172))\n",
        "\n",
        "    new_df.to_csv(OUT_DF)\n",
        "    np.save(OUT_FEAT, feat)\n",
        "    np.save(OUT_NODE_FEAT, rand_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b991bc9d-c2c6-4bac-81d1-0e5e698f3d27",
      "metadata": {
        "id": "b991bc9d-c2c6-4bac-81d1-0e5e698f3d27"
      },
      "outputs": [],
      "source": [
        "def TemporalDataset(dataset, force_reload = False):\n",
        "    if force_reload or not os.path.exists('./data/{}.bin'.format(dataset)):\n",
        "        if not os.path.exists('./data/{}.csv'.format(dataset)):\n",
        "            if not os.path.exists('./data'):\n",
        "                os.mkdir('./data')\n",
        "\n",
        "            url = 'https://snap.stanford.edu/jodie/{}.csv'.format(dataset)\n",
        "            print(\"Start Downloading File....\")\n",
        "            context = ssl._create_unverified_context()\n",
        "            data = urllib.request.urlopen(url, context=context)\n",
        "            with open(\"./data/{}.csv\".format(dataset), \"wb\") as handle:\n",
        "                handle.write(data.read())\n",
        "\n",
        "        print(\"Start Process Data ...\")\n",
        "        run(dataset)\n",
        "        raw_connection = pd.read_csv('./data/ml_{}.csv'.format(dataset))\n",
        "        raw_feature = np.load('./data/ml_{}.npy'.format(dataset))\n",
        "        # -1 for re-index the node\n",
        "        src = raw_connection['u'].to_numpy()-1\n",
        "        dst = raw_connection['i'].to_numpy()-1\n",
        "        # Create directed graph\n",
        "        g = dgl.graph((src, dst))\n",
        "        g.edata['timestamp'] = torch.from_numpy(\n",
        "            raw_connection['ts'].to_numpy())\n",
        "        g.edata['label'] = torch.from_numpy(raw_connection['label'].to_numpy())\n",
        "        g.edata['feats'] = torch.from_numpy(raw_feature[1:, :]).float()\n",
        "        g.ndata[dgl.NID] = g.nodes()\n",
        "        dgl.save_graphs('./data/{}.bin'.format(dataset), [g])\n",
        "    else:\n",
        "        print(\"Data is exist directly loaded.\")\n",
        "        gs, _ = dgl.load_graphs('./data/{}.bin'.format(dataset))\n",
        "        g = gs[0]\n",
        "    return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf0a64f-1837-4708-9d12-36f9cb24af6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bf0a64f-1837-4708-9d12-36f9cb24af6d",
        "outputId": "d0e8b8db-678c-41d6-973d-25c5a035aeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Process Data ...\n"
          ]
        }
      ],
      "source": [
        "# data =  TemporalDataset('wikipedia', force_reload=False)\n",
        "data =  TemporalDataset(data_file, force_reload=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fffee2cb-8043-40d6-a4f9-2b3f58c93f3b",
      "metadata": {
        "id": "fffee2cb-8043-40d6-a4f9-2b3f58c93f3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f31a72-f895-4ed1-8e45-338b3fe82673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   0,    1,    2,  ..., 9224, 9225, 9226])\n",
            "(tensor([   0,    1,    1,  ..., 2399, 7479, 2399]), tensor([8227, 8228, 8228,  ..., 8722, 9147, 8722]))\n"
          ]
        }
      ],
      "source": [
        "# data = data.int()\n",
        "print(data.ndata[dgl.NID])\n",
        "print(data.edges())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc84fb6-6d8c-4769-aed4-b6cd56302545",
      "metadata": {
        "id": "ccc84fb6-6d8c-4769-aed4-b6cd56302545"
      },
      "outputs": [],
      "source": [
        "# print(data.edges())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76fa216a-82e1-4f73-94e7-a7b6fcf765f1",
      "metadata": {
        "id": "76fa216a-82e1-4f73-94e7-a7b6fcf765f1"
      },
      "outputs": [],
      "source": [
        "# print(data.edata['feats'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ee70d91-6bc2-4f9b-8190-74bc101af2d6",
      "metadata": {
        "id": "3ee70d91-6bc2-4f9b-8190-74bc101af2d6"
      },
      "outputs": [],
      "source": [
        "# print(data.edata['timestamp'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf7dd8b-45fe-4c58-84c3-07ebf76c3820",
      "metadata": {
        "id": "6bf7dd8b-45fe-4c58-84c3-07ebf76c3820"
      },
      "outputs": [],
      "source": [
        "# class WikiDataset(dgl.data.DGLDataset():\n",
        "    # __init__(self):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1789c71b-cebe-4b13-9965-ba5dd18bcb04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1789c71b-cebe-4b13-9965-ba5dd18bcb04",
        "outputId": "c3b42e67-5a30-4786-cd18-8085c2fcbe29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7908d13e7290>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "num_nodes = data.num_nodes()\n",
        "num_edges = data.num_edges()\n",
        "TRAIN_SPLIT = 0.7\n",
        "VALID_SPLIT = 0.85\n",
        "\n",
        "# set random Seed\n",
        "np.random.seed(2021)\n",
        "torch.manual_seed(2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c3ae3d-b5f6-4162-8c6f-b87c181adf14",
      "metadata": {
        "id": "a0c3ae3d-b5f6-4162-8c6f-b87c181adf14"
      },
      "outputs": [],
      "source": [
        "# print(num_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea242502-6147-4c05-bffe-2dbae395581e",
      "metadata": {
        "id": "ea242502-6147-4c05-bffe-2dbae395581e"
      },
      "outputs": [],
      "source": [
        "trainval_div = int(VALID_SPLIT*num_edges)\n",
        "# print(trainval_div)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cedbc147-8ae8-4485-9164-118238158461",
      "metadata": {
        "id": "cedbc147-8ae8-4485-9164-118238158461"
      },
      "outputs": [],
      "source": [
        "test_split_ts = data.edata['timestamp'][trainval_div]\n",
        "# print(test_split_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a046b4e3-257f-4b87-941a-f47e180a8ca7",
      "metadata": {
        "id": "a046b4e3-257f-4b87-941a-f47e180a8ca7"
      },
      "outputs": [],
      "source": [
        "test_nodes = torch.cat([data.edges()[0][trainval_div:], data.edges()[1][trainval_div:]]).unique().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa68ea3-5348-4bda-9916-dcb96af5eb2c",
      "metadata": {
        "id": "9aa68ea3-5348-4bda-9916-dcb96af5eb2c"
      },
      "outputs": [],
      "source": [
        "test_new_nodes = np.random.choice(test_nodes, int(0.1*len(test_nodes)), replace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6e28f07-42cd-4683-8ede-16385f7fe72f",
      "metadata": {
        "id": "c6e28f07-42cd-4683-8ede-16385f7fe72f"
      },
      "outputs": [],
      "source": [
        "# print(test_nodes.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1b21418-6bd3-4560-a52c-f3c6983a52e4",
      "metadata": {
        "id": "b1b21418-6bd3-4560-a52c-f3c6983a52e4"
      },
      "outputs": [],
      "source": [
        "# print(test_new_nodes.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5392b636-6035-4561-8d87-befa077ad60d",
      "metadata": {
        "id": "5392b636-6035-4561-8d87-befa077ad60d"
      },
      "outputs": [],
      "source": [
        "in_subg = dgl.in_subgraph(data, test_new_nodes)\n",
        "out_subg = dgl.out_subgraph(data, test_new_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5effc74-cf03-4466-8617-0b1e0c058d34",
      "metadata": {
        "id": "f5effc74-cf03-4466-8617-0b1e0c058d34"
      },
      "outputs": [],
      "source": [
        "# print(len(in_subg.nodes()))\n",
        "# print(in_subg.edges())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49a02a4-2673-4b28-87aa-1d50c4e05605",
      "metadata": {
        "id": "a49a02a4-2673-4b28-87aa-1d50c4e05605"
      },
      "outputs": [],
      "source": [
        "# Remove edge who happen before the test set to prevent from learning the connection info\n",
        "new_node_in_eid_delete = in_subg.edata[dgl.EID][in_subg.edata['timestamp'] < test_split_ts]\n",
        "#gets the eids in in_sub_g that has time_stamp lower than\n",
        "# print(test_split_ts)\n",
        "# print(in_subg.edata['timestamp'] < test_split_ts)\n",
        "# print(in_subg.edata[dgl.EID])\n",
        "# print(new_node_in_eid_delete)\n",
        "new_node_out_eid_delete = out_subg.edata[dgl.EID][out_subg.edata['timestamp'] < test_split_ts]\n",
        "#all the inbound and outbound edges that occurred before test_split_ts\n",
        "new_node_eid_delete = torch.cat([new_node_in_eid_delete, new_node_out_eid_delete]).unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61126719-7b3a-46ab-8bb5-85494f64f532",
      "metadata": {
        "id": "61126719-7b3a-46ab-8bb5-85494f64f532"
      },
      "outputs": [],
      "source": [
        "graph_new_node = copy.deepcopy(data)\n",
        "graph_new_node.remove_edges(new_node_eid_delete)\n",
        "#Here we have removed only the edges that occur before test_split_ts and have node_id\n",
        "#belonging to test_split new nodes. In this way there will be no\n",
        "#edge in the train or val split these edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf249bb8-9d2e-4ec4-ac51-a5d69d2af8e0",
      "metadata": {
        "id": "cf249bb8-9d2e-4ec4-ac51-a5d69d2af8e0"
      },
      "outputs": [],
      "source": [
        "# print(dgl.NID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82614e1e-ea9b-4763-a7ad-6646859e5ef0",
      "metadata": {
        "id": "82614e1e-ea9b-4763-a7ad-6646859e5ef0"
      },
      "outputs": [],
      "source": [
        "# Now for no new node graph, all edge id need to be removed\n",
        "in_eid_delete = in_subg.edata[dgl.EID]\n",
        "out_eid_delete = out_subg.edata[dgl.EID]\n",
        "eid_delete = torch.cat([in_eid_delete, out_eid_delete]).unique()\n",
        "\n",
        "graph_no_new_node = copy.deepcopy(data)\n",
        "graph_no_new_node.remove_edges(eid_delete)\n",
        "#Here graph_no_new_node has no edges for those selected nodes ib train, test or valid split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fda2731-3c98-4507-8ac1-d2a85537210c",
      "metadata": {
        "id": "4fda2731-3c98-4507-8ac1-d2a85537210c"
      },
      "outputs": [],
      "source": [
        "# print(graph_no_new_node.nodes())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf1d257f-d9d4-48c3-8f24-338e8860bd4c",
      "metadata": {
        "id": "bf1d257f-d9d4-48c3-8f24-338e8860bd4c"
      },
      "outputs": [],
      "source": [
        "# in_nid = in_subg.edata[dgl.NID]\n",
        "# print(in_subg.edata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c26070-8d4a-4c71-b911-645edf2105af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40c26070-8d4a-4c71-b911-645edf2105af",
        "outputId": "c773784d-e9dd-4e65-dca8-811f293f42b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134309\n",
            "268618\n",
            "139206\n",
            "278412\n"
          ]
        }
      ],
      "source": [
        "neg_sampler = dgl.dataloading.negative_sampler.Uniform(k=args.num_negative_samples)\n",
        "g_sampling = None if args.fast_mode else dgl.add_reverse_edges(graph_no_new_node, copy_edata=True)\n",
        "print(graph_no_new_node.num_edges())\n",
        "print(g_sampling.num_edges())\n",
        "new_node_g_sampling = None if args.fast_mode else dgl.add_reverse_edges(graph_new_node, copy_edata=True)\n",
        "print(graph_new_node.num_edges())\n",
        "print(new_node_g_sampling.num_edges())\n",
        "#     if not args.fast_mode:\n",
        "#         new_node_g_sampling.ndata[dgl.NID] = new_node_g_sampling.nodes()\n",
        "#         g_sampling.ndata[dgl.NID] = new_node_g_sampling.nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60f3615-2e31-4c1d-9e2d-0e4116f0c914",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c60f3615-2e31-4c1d-9e2d-0e4116f0c914",
        "outputId": "ed6de307-1d22-466a-a6d8-cb63cf24315f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    0,     1,     2,  ..., 94013, 94014, 94015])\n",
            "tensor([ 94016,  94017,  94018,  ..., 115581, 115582, 115583])\n",
            "tensor([115584, 115585, 115586,  ..., 134306, 134307, 134308])\n",
            "tensor([115584, 115585, 115586,  ..., 139203, 139204, 139205])\n"
          ]
        }
      ],
      "source": [
        "# Set Train, validation, test and new node test id\n",
        "train_seed = torch.arange(int(TRAIN_SPLIT*graph_no_new_node.num_edges()))\n",
        "print(train_seed)\n",
        "valid_seed = torch.arange(int(TRAIN_SPLIT*graph_no_new_node.num_edges()), trainval_div-new_node_eid_delete.size(0))\n",
        "print(valid_seed)\n",
        "test_seed = torch.arange(trainval_div-new_node_eid_delete.size(0), graph_no_new_node.num_edges())\n",
        "print(test_seed)\n",
        "test_new_node_seed = torch.arange(trainval_div-new_node_eid_delete.size(0), graph_new_node.num_edges())\n",
        "print(test_new_node_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "715b2642-a56a-4eb7-af43-903d8d3c72a7",
      "metadata": {
        "id": "715b2642-a56a-4eb7-af43-903d8d3c72a7"
      },
      "outputs": [],
      "source": [
        "# print(new_node_g_sampling.nodes())\n",
        "# print(g_sampling.ndata[dgl.NID] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd5354f-faf1-4251-8dc6-758a408196f7",
      "metadata": {
        "id": "6fd5354f-faf1-4251-8dc6-758a408196f7"
      },
      "outputs": [],
      "source": [
        "from dgl.dataloading import BlockSampler\n",
        "from functools import partial\n",
        "class TemporalSampler(BlockSampler):\n",
        "    \"\"\" Temporal Sampler builds computational and temporal dependency of node representations via\n",
        "    temporal neighbors selection and screening.\n",
        "\n",
        "    The sampler expects input node to have same time stamps, in the case of TGN, it should be\n",
        "    either positive [src,dst] pair or negative samples. It will first take in-subgraph of seed\n",
        "    nodes and then screening out edges which happen after that timestamp. Finally it will sample\n",
        "    a fixed number of neighbor edges using random or topk sampling.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sampler_type : str\n",
        "        sampler indication string of the final sampler.\n",
        "\n",
        "        If 'topk' then sample topk most recent nodes\n",
        "\n",
        "        If 'uniform' then uniform randomly sample k nodes\n",
        "\n",
        "    k : int\n",
        "        maximum number of neighors to sampler\n",
        "\n",
        "        default 10 neighbors as paper stated\n",
        "\n",
        "    Examples\n",
        "    ----------\n",
        "    Please refers to examples/pytorch/tgn/train.py\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sampler_type='topk', k=10):\n",
        "        super(TemporalSampler, self).__init__()\n",
        "        if sampler_type == 'topk':\n",
        "            self.sampler = partial(\n",
        "                dgl.sampling.select_topk, k=k, weight='timestamp')\n",
        "        elif sampler_type == 'uniform':\n",
        "            self.sampler = partial(dgl.sampling.sample_neighbors, fanout=k)\n",
        "        else:\n",
        "            raise DGLError(\n",
        "                \"Sampler string invalid please use \\'topk\\' or \\'uniform\\'\")\n",
        "\n",
        "    def sample(\n",
        "        self, g, seed_nodes, exclude_eids=None, timestamp = 0,\n",
        "    ):  # pylint: disable=arguments-differ\n",
        "        \"\"\"Sample a list of blocks from the given seed nodes.\"\"\"\n",
        "        result = self.sample_blocks(g, seed_nodes, timestamp, exclude_eids=exclude_eids)\n",
        "        return self.assign_lazy_features(result)\n",
        "\n",
        "    def sampler_frontier(self,\n",
        "                         block_id,\n",
        "                         g,\n",
        "                         seed_nodes,\n",
        "                         timestamp):\n",
        "        full_neighbor_subgraph = dgl.in_subgraph(g, seed_nodes)\n",
        "        # print(\"Full NSG1: \", full_neighbor_subgraph)\n",
        "        # print(\"seed nodes: \", seed_nodes)\n",
        "\n",
        "        # Adding self loops? but why?\n",
        "        full_neighbor_subgraph = dgl.add_edges(full_neighbor_subgraph,\n",
        "                                               seed_nodes, seed_nodes)\n",
        "        # print(\"full_neighbor_subgraph edges: \", full_neighbor_subgraph.edges())\n",
        "        # print(\"Full NSG2: \", full_neighbor_subgraph)\n",
        "\n",
        "        #Remove edges that occurs after timetsamp\n",
        "        temporal_edge_mask = (full_neighbor_subgraph.edata['timestamp'] < timestamp) + (\n",
        "            full_neighbor_subgraph.edata['timestamp'] <= 0)\n",
        "        # print(\"temporal_edge_mask: \", temporal_edge_mask)\n",
        "        temporal_subgraph = dgl.edge_subgraph(\n",
        "            full_neighbor_subgraph, temporal_edge_mask)\n",
        "\n",
        "        # Map preserve ID\n",
        "        temp2origin = temporal_subgraph.ndata[dgl.NID]\n",
        "#         print(\"seed nodes: \", seed_nodes)\n",
        "#         print(\"temp2origin (actual node ids): \", temp2origin)\n",
        "#         print(\"temporal_subgraph edges: \", temporal_subgraph.edges())\n",
        "#         print(\"temporal subgraph nids: \", temporal_subgraph.ndata[dgl.NID])\n",
        "\n",
        "\n",
        "        # The added new edgge will be preserved hence\n",
        "        root2sub_dict = dict(\n",
        "            zip(temp2origin.tolist(), temporal_subgraph.nodes().tolist()))\n",
        "\n",
        "\n",
        "        # temporal_subgraph.ndata[\"_orgID\"] = g.ndata[dgl.NID][temp2origin]\n",
        "        # print(\"temporal subgraph nids: \", temporal_subgraph.ndata[dgl.NID])\n",
        "        # print(\"seed node before updates: \", seed_nodes)\n",
        "        seed_nodes = [root2sub_dict[int(n)] for n in seed_nodes]\n",
        "\n",
        "        # print(\"updated seed nodes: \", seed_nodes)\n",
        "        # print(\"\")\n",
        "        final_subgraph = self.sampler(g=temporal_subgraph, nodes=seed_nodes)\n",
        "        final_subgraph.remove_self_loop()\n",
        "        # print(\"final subgraphs: \", final_subgraph)\n",
        "        # print(\"Eids:\", final_subgraph.edges())\n",
        "        # print(\"Nids:\", final_subgraph.nodes())\n",
        "        # print(\"ndata: \", final_subgraph.ndata)\n",
        "        src_nodes, dest_nodes =  final_subgraph.edges()\n",
        "        src_nodes = src_nodes.unique()\n",
        "        dest_nodes = dest_nodes.unique()\n",
        "        # print(\"s: \", src_nodes)\n",
        "        # print(\"d: \", dest_nodes)\n",
        "        block = dgl.transforms.to_block(final_subgraph, seed_nodes)\n",
        "        block2 = dgl.transforms.to_block(final_subgraph)\n",
        "\n",
        "#         print(\"block: \", block)\n",
        "#         print(\"block ndata: \", block.ndata)\n",
        "#         print(\"block srcdata: \", block.srcdata)\n",
        "#         print(\"block dstdata: \", block.dstdata)\n",
        "\n",
        "        # print(\"block2: \", block2)\n",
        "        # print(\"block2 ndata: \", block2.ndata)\n",
        "        # print(\"block2 srcdata: \", block2.srcdata)\n",
        "        # print(\"block2 dstdata: \", block2.dstdata)\n",
        "        return src_nodes, dest_nodes, block #dgl.transforms.to_block(final_subgraph)\n",
        "\n",
        "\n",
        "\n",
        "    def sampler_frontier_for_Batch(self,\n",
        "                         block_id,\n",
        "                         g,\n",
        "                         seed_nodes,\n",
        "                         timestamp):\n",
        "        full_neighbor_subgraph = dgl.in_subgraph(g, seed_nodes)\n",
        "        # print(\"Full NSG1: \", full_neighbor_subgraph)\n",
        "        # print(\"seed nodes: \", seed_nodes)\n",
        "\n",
        "        # Adding self loops? but why?\n",
        "        full_neighbor_subgraph = dgl.add_edges(full_neighbor_subgraph,\n",
        "                                               seed_nodes, seed_nodes)\n",
        "        # print(\"full_neighbor_subgraph edges: \", full_neighbor_subgraph.edges())\n",
        "        # print(\"Full NSG2: \", full_neighbor_subgraph)\n",
        "\n",
        "        #Remove edges that occurs after timetsamp\n",
        "        temporal_edge_mask = (full_neighbor_subgraph.edata['timestamp'] < timestamp) + (\n",
        "            full_neighbor_subgraph.edata['timestamp'] <= 0)\n",
        "        # print(\"temporal_edge_mask: \", temporal_edge_mask)\n",
        "        temporal_subgraph = dgl.edge_subgraph(\n",
        "            full_neighbor_subgraph, temporal_edge_mask)\n",
        "\n",
        "        # Map preserve ID\n",
        "        temp2origin = temporal_subgraph.ndata[dgl.NID]\n",
        "#         print(\"seed nodes: \", seed_nodes)\n",
        "#         print(\"temp2origin (actual node ids): \", temp2origin)\n",
        "#         print(\"temporal_subgraph edges: \", temporal_subgraph.edges())\n",
        "#         print(\"temporal subgraph nids: \", temporal_subgraph.ndata[dgl.NID])\n",
        "\n",
        "\n",
        "        # The added new edgge will be preserved hence\n",
        "        root2sub_dict = dict(\n",
        "            zip(temp2origin.tolist(), temporal_subgraph.nodes().tolist()))\n",
        "\n",
        "\n",
        "        temporal_subgraph.ndata[\"_orgID\"] = g.ndata[dgl.NID][temp2origin]\n",
        "        # print(\"temporal subgraph nids: \", temporal_subgraph.ndata[dgl.NID])\n",
        "        # print(\"seed node before updates: \", seed_nodes)\n",
        "        seed_nodes = [root2sub_dict[int(n)] for n in seed_nodes]\n",
        "\n",
        "        # print(\"updated seed nodes: \", seed_nodes)\n",
        "        # print(\"\")\n",
        "        final_subgraph = self.sampler(g=temporal_subgraph, nodes=seed_nodes)\n",
        "        final_subgraph.remove_self_loop()\n",
        "        # print(\"final subgraphs: \", final_subgraph)\n",
        "        # print(\"Eids:\", final_subgraph.edges())\n",
        "        # print(\"Nids:\", final_subgraph.nodes())\n",
        "        # print(\"ndata: \", final_subgraph.ndata)\n",
        "        src_nodes, dest_nodes =  final_subgraph.edges()\n",
        "        src_nodes = src_nodes.unique()\n",
        "        dest_nodes = dest_nodes.unique()\n",
        "        # print(\"s: \", src_nodes)\n",
        "        # print(\"d: \", dest_nodes)\n",
        "        # block = dgl.transforms.to_block(final_subgraph, seed_nodes)\n",
        "        # block2 = dgl.transforms.to_block(final_subgraph)\n",
        "\n",
        "#         print(\"block: \", block)\n",
        "#         print(\"block ndata: \", block.ndata)\n",
        "#         print(\"block srcdata: \", block.srcdata)\n",
        "#         print(\"block dstdata: \", block.dstdata)\n",
        "\n",
        "        # print(\"block2: \", block2)\n",
        "        # print(\"block2 ndata: \", block2.ndata)\n",
        "        # print(\"block2 srcdata: \", block2.srcdata)\n",
        "        # print(\"block2 dstdata: \", block2.dstdata)\n",
        "        return src_nodes, dest_nodes, final_subgraph #dgl.transforms.to_block(final_subgraph)\n",
        "\n",
        "        # Temporal Subgraph\n",
        "\n",
        "    def sample_blocks(self,\n",
        "                      g,\n",
        "                      seed_nodes, timestamp,\n",
        "                      exclude_eids=None):\n",
        "        blocks = []\n",
        "        s, d, frontier = self.sampler_frontier(0, g, seed_nodes, timestamp)\n",
        "        #block = transform.to_block(frontier,seed_nodes)\n",
        "        block = frontier\n",
        "        # if self.return_eids:\n",
        "        #     self.assign_block_eids(block, frontier)\n",
        "        blocks.append(block)\n",
        "        # print(blocks)\n",
        "        # # g.srcnodes, g.srcdata, feature_names\n",
        "        # print(\"block src nodes: \", block.srcnodes)\n",
        "        # print(\"block src data: \", block.srcdata)\n",
        "        # # print(\"node feats prefetch: \", self.prefetch_node_feats)\n",
        "        return (s, d, blocks)\n",
        "    def sample_blocks_for_Batch(self,\n",
        "                      g,\n",
        "                      seed_nodes, timestamp,\n",
        "                      exclude_eids=None):\n",
        "        blocks = []\n",
        "        s, d, frontier = self.sampler_frontier_for_Batch(0, g, seed_nodes, timestamp)\n",
        "        #block = transform.to_block(frontier,seed_nodes)\n",
        "        block = frontier\n",
        "        # if self.return_eids:\n",
        "        #     self.assign_block_eids(block, frontier)\n",
        "        blocks.append(block)\n",
        "        # print(blocks)\n",
        "        # # g.srcnodes, g.srcdata, feature_names\n",
        "        # print(\"block src nodes: \", block.srcnodes)\n",
        "        # print(\"block src data: \", block.srcdata)\n",
        "        # # print(\"node feats prefetch: \", self.prefetch_node_feats)\n",
        "        return (s, d, blocks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c034d3-7ecc-4bc8-bc88-bc5657aa9354",
      "metadata": {
        "id": "54c034d3-7ecc-4bc8-bc88-bc5657aa9354"
      },
      "outputs": [],
      "source": [
        "from dgl.dataloading import Sampler\n",
        "from dgl.dataloading import EdgePredictionSampler, set_edge_lazy_features, compact_graphs, Mapping, EID, NID, F, context_of, heterograph, find_exclude_eids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499d8170-fb0c-4ee3-b602-c6cf2774cf4d",
      "metadata": {
        "id": "499d8170-fb0c-4ee3-b602-c6cf2774cf4d"
      },
      "outputs": [],
      "source": [
        "# from dgl.dataloading import Sampler\n",
        "class TemporalEdgePredictionSampler(Sampler):\n",
        "    \"\"\"Sampler class that wraps an existing sampler for node classification into another\n",
        "    one for edge classification or link prediction.\n",
        "\n",
        "    See also\n",
        "    --------\n",
        "    as_edge_predIt finds all the nodes that have zero in-degree and zero out-degree in all the given graphs, and eliminates them from all the graphs.iction_sampler\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sampler,\n",
        "        exclude=None,\n",
        "        reverse_eids=None,\n",
        "        reverse_etypes=None,\n",
        "        negative_sampler=None,\n",
        "        prefetch_labels=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Check if the sampler's sample method has an optional third argument.\n",
        "        argspec = inspect.getfullargspec(sampler.sample)\n",
        "        if len(argspec.args) < 4:  # ['self', 'g', 'indices', 'exclude_eids']\n",
        "            raise TypeError(\n",
        "                \"This sampler does not support edge or link prediction; please add an\"\n",
        "                \"optional third argument for edge IDs to exclude in its sample() method.\"\n",
        "            )\n",
        "        self.reverse_eids = reverse_eids\n",
        "        self.reverse_etypes = reverse_etypes\n",
        "        self.exclude = exclude\n",
        "        self.sampler = sampler\n",
        "        self.negative_sampler = negative_sampler\n",
        "        self.prefetch_labels = prefetch_labels or []\n",
        "        self.output_device = sampler.output_device\n",
        "\n",
        "    def _build_neg_graph(self, g, seed_edges):\n",
        "        neg_srcdst = self.negative_sampler(g, seed_edges)\n",
        "        if not isinstance(neg_srcdst, Mapping):\n",
        "            assert len(g.canonical_etypes) == 1, (\n",
        "                \"graph has multiple or no edge types; \"\n",
        "                \"please return a dict in negative sampler.\"\n",
        "            )\n",
        "            neg_srcdst = {g.canonical_etypes[0]: neg_srcdst}\n",
        "\n",
        "        dtype = F.dtype(list(neg_srcdst.values())[0][0])\n",
        "        ctx = context_of(seed_edges) if seed_edges is not None else g.device\n",
        "        neg_edges = {\n",
        "            etype: neg_srcdst.get(\n",
        "                etype,\n",
        "                (\n",
        "                    F.copy_to(F.tensor([], dtype), ctx=ctx),\n",
        "                    F.copy_to(F.tensor([], dtype), ctx=ctx),\n",
        "                ),\n",
        "            )\n",
        "            for etype in g.canonical_etypes\n",
        "        }\n",
        "        neg_pair_graph = heterograph(\n",
        "            neg_edges, {ntype: g.num_nodes(ntype) for ntype in g.ntypes}\n",
        "        )\n",
        "        return neg_pair_graph\n",
        "\n",
        "    def assign_lazy_features(self, result):\n",
        "        \"\"\"Assign lazy features for prefetching.\"\"\"\n",
        "        pair_graph = result[1]\n",
        "        set_edge_lazy_features(pair_graph, self.prefetch_labels)\n",
        "        # In-place updates\n",
        "        return result\n",
        "\n",
        "    def sample(self, g, seed_edges):  # pylint: disable=arguments-differ\n",
        "        \"\"\"Samples a list of blocks, as well as a subgraph containing the sampled\n",
        "        edges from the original graph.\n",
        "\n",
        "        If :attr:`negative_sampler` is given, also returns another graph containing the\n",
        "        negative pairs as edges.\n",
        "        \"\"\"\n",
        "        # print(\"seed_edges: \", seed_edges)\n",
        "        # print(\"time stamp of the seed edges: \", g.edata['timestamp'][seed_edges])\n",
        "        timestamps = g.edata['timestamp'][seed_edges]\n",
        "        if isinstance(seed_edges, Mapping):\n",
        "            seed_edges = {\n",
        "                g.to_canonical_etype(k): v for k, v in seed_edges.items()\n",
        "            }\n",
        "        exclude = self.exclude\n",
        "        pair_graph = g.edge_subgraph(\n",
        "            seed_edges, relabel_nodes=False, output_device=self.output_device\n",
        "        )\n",
        "        # print(\"pair graph edges: \",pair_graph.edges())\n",
        "\n",
        "        eids = pair_graph.edata[EID]\n",
        "        # print(\"Pair graph: \", pair_graph)\n",
        "\n",
        "        if self.negative_sampler is not None:\n",
        "            neg_graph = self._build_neg_graph(g, seed_edges)\n",
        "            pair_graph, neg_graph = compact_graphs([pair_graph, neg_graph])\n",
        "        else:\n",
        "            pair_graph = compact_graphs(pair_graph)\n",
        "\n",
        "        pair_graph.edata[EID] = eids\n",
        "        seed_nodes = pair_graph.ndata[NID]\n",
        "\n",
        "#         print(\"Pair graph: \", pair_graph)\n",
        "#         print(\"Nodes: \", pair_graph.nodes())\n",
        "#         print(\"Ndata: \", pair_graph.ndata)\n",
        "#         print(\"Edges: \", pair_graph.edges())\n",
        "#         print(\"Edata:\", pair_graph.edata)\n",
        "\n",
        "#         print(\"Neg graph: \", neg_graph)\n",
        "#         print(\"Nodes: \", neg_graph.nodes())\n",
        "#         print(\"Ndata: \", neg_graph.ndata)\n",
        "#         print(\"Edges: \", neg_graph.edges())\n",
        "#         print(\"Edata:\", neg_graph.edata)\n",
        "\n",
        "\n",
        "        exclude_eids = find_exclude_eids(\n",
        "            g,\n",
        "            seed_edges,\n",
        "            exclude,\n",
        "            self.reverse_eids,\n",
        "            self.reverse_etypes,\n",
        "            self.output_device,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        #time_stamp of the first seed edge\n",
        "        # print(\"seed edges: \", see)\n",
        "        # timestamp =\n",
        "        input_nodes, _, blocks = self.sampler.sample(g, seed_nodes, exclude_eids, timestamp = timestamps[0])\n",
        "\n",
        "        if self.negative_sampler is None:\n",
        "            return self.assign_lazy_features((input_nodes, pair_graph, blocks))\n",
        "        else:\n",
        "            return self.assign_lazy_features(\n",
        "                (input_nodes, pair_graph, neg_graph, blocks)\n",
        "            )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631f1629-c8ad-4dcf-8a08-4b8fcc3570b3",
      "metadata": {
        "id": "631f1629-c8ad-4dcf-8a08-4b8fcc3570b3"
      },
      "outputs": [],
      "source": [
        "# from dgl.dataloading import Sampler\n",
        "class BatchedTemporalEdgePredictionSampler(Sampler):\n",
        "    \"\"\"Sampler class that wraps an existing sampler for node classification into another\n",
        "    one for edge classification or link prediction.\n",
        "\n",
        "    See also\n",
        "    --------\n",
        "    as_edge_predIt finds all the nodes that have zero in-degree and zero out-degree in all the given graphs, and eliminates them from all the graphs.iction_sampler\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sampler,\n",
        "        exclude=None,\n",
        "        reverse_eids=None,\n",
        "        reverse_etypes=None,\n",
        "        negative_sampler=None,\n",
        "        prefetch_labels=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Check if the sampler's sample method has an optional third argument.\n",
        "        argspec = inspect.getfullargspec(sampler.sample)\n",
        "        if len(argspec.args) < 4:  # ['self', 'g', 'indices', 'exclude_eids']\n",
        "            raise TypeError(\n",
        "                \"This sampler does not support edge or link prediction; please add an\"\n",
        "                \"optional third argument for edge IDs to exclude in its sample() method.\"\n",
        "            )\n",
        "        self.reverse_eids = reverse_eids\n",
        "        self.reverse_etypes = reverse_etypes\n",
        "        self.exclude = exclude\n",
        "        self.sampler = sampler\n",
        "        self.negative_sampler = negative_sampler\n",
        "        self.prefetch_labels = prefetch_labels or []\n",
        "        self.output_device = sampler.output_device\n",
        "\n",
        "    def _build_neg_graph(self, g, seed_edges):\n",
        "        neg_srcdst = self.negative_sampler(g, seed_edges)\n",
        "        if not isinstance(neg_srcdst, Mapping):\n",
        "            assert len(g.canonical_etypes) == 1, (\n",
        "                \"graph has multiple or no edge types; \"\n",
        "                \"please return a dict in negative sampler.\"\n",
        "            )\n",
        "            neg_srcdst = {g.canonical_etypes[0]: neg_srcdst}\n",
        "\n",
        "        dtype = F.dtype(list(neg_srcdst.values())[0][0])\n",
        "        ctx = context_of(seed_edges) if seed_edges is not None else g.device\n",
        "        neg_edges = {\n",
        "            etype: neg_srcdst.get(\n",
        "                etype,\n",
        "                (\n",
        "                    F.copy_to(F.tensor([], dtype), ctx=ctx),\n",
        "                    F.copy_to(F.tensor([], dtype), ctx=ctx),\n",
        "                ),\n",
        "            )\n",
        "            for etype in g.canonical_etypes\n",
        "        }\n",
        "        neg_pair_graph = heterograph(\n",
        "            neg_edges, {ntype: g.num_nodes(ntype) for ntype in g.ntypes}\n",
        "        )\n",
        "        return neg_pair_graph\n",
        "\n",
        "    def assign_lazy_features(self, result):\n",
        "        \"\"\"Assign lazy features for prefetching.\"\"\"\n",
        "        pair_graph = result[1]\n",
        "        set_edge_lazy_features(pair_graph, self.prefetch_labels)\n",
        "        # In-place updates\n",
        "        return result\n",
        "\n",
        "    def sample(self, g, seed_edges):  # pylint: disable=arguments-differ\n",
        "        \"\"\"Samples a list of blocks, as well as a subgraph containing the sampled\n",
        "        edges from the original graph.\n",
        "\n",
        "        If :attr:`negative_sampler` is given, also returns another graph containing the\n",
        "        negative pairs as edges.\n",
        "        \"\"\"\n",
        "        # print(\"seed_edges: \", seed_edges)\n",
        "        # print(\"g edges:\", g.edges())\n",
        "        # print(\"g ndata: \", g.ndata)\n",
        "        # print(\"time stamp of the seed edges: \", g.edata['timestamp'][seed_edges])\n",
        "        timestamps = g.edata['timestamp'][seed_edges]\n",
        "        if isinstance(seed_edges, Mapping):\n",
        "            seed_edges = {\n",
        "                g.to_canonical_etype(k): v for k, v in seed_edges.items()\n",
        "            }\n",
        "        exclude = self.exclude\n",
        "        pair_graph = g.edge_subgraph(\n",
        "            seed_edges, relabel_nodes=False, output_device=self.output_device\n",
        "        )\n",
        "        # print(seed_edges)\n",
        "        # print(\"pair graph ndata: \",pair_graph.ndata)\n",
        "        # print(\"edges: \", pair_graph.edges())\n",
        "\n",
        "        eids = pair_graph.edata[EID]\n",
        "        # print(\"Pair graph: \", pair_graph)\n",
        "\n",
        "        if self.negative_sampler is not None:\n",
        "            neg_graph = self._build_neg_graph(g, seed_edges)\n",
        "            # print(\"neg_graph ndata: \", neg_graph.ndata)\n",
        "            # print(\"neg_graph edges: \", neg_graph.edges())\n",
        "            pair_graph, neg_graph = compact_graphs([pair_graph, neg_graph])\n",
        "            # print(\"compact neg_graph ndata: \", neg_graph.ndata)\n",
        "\n",
        "        else:\n",
        "            pair_graph = compact_graphs(pair_graph)\n",
        "\n",
        "        # print(\"compact pair graph ndata: \",pair_graph.ndata)\n",
        "        # print(\"edges: \", pair_graph.edges())\n",
        "        pair_graph.edata[EID] = eids\n",
        "        seed_nodes = pair_graph.ndata[NID]\n",
        "\n",
        "#         print(\"Pair graph: \", pair_graph)\n",
        "#         print(\"Nodes: \", pair_graph.nodes())\n",
        "#         print(\"Ndata: \", pair_graph.ndata)\n",
        "#         print(\"Edges: \", pair_graph.edges())\n",
        "#         print(\"Edata:\", pair_graph.edata)\n",
        "\n",
        "#         print(\"Neg graph: \", neg_graph)\n",
        "#         print(\"Nodes: \", neg_graph.nodes())\n",
        "#         print(\"Ndata: \", neg_graph.ndata)\n",
        "#         print(\"Edges: \", neg_graph.edges())\n",
        "#         print(\"Edata:\", neg_graph.edata)\n",
        "\n",
        "\n",
        "        batch_graphs = []\n",
        "        nodes_id = []\n",
        "        timestamps = []\n",
        "        exclude_eids = find_exclude_eids(\n",
        "            g,\n",
        "            seed_edges,\n",
        "            exclude,\n",
        "            self.reverse_eids,\n",
        "            self.reverse_etypes,\n",
        "            self.output_device,\n",
        "        )\n",
        "        for i, edge in enumerate(zip(g.edges()[0][seed_edges], g.edges()[1][seed_edges])):\n",
        "            # ts = pair_graph.edata['timestamp'][i]\n",
        "            ts = pair_graph.edata['timestamp'][i]\n",
        "            timestamps.append(ts)\n",
        "            _, _, subgs = self.sampler.sample_blocks_for_Batch(g,list(edge),timestamp=ts)\n",
        "            subg = subgs[0]\n",
        "#             print(\"subg: \",  subg)\n",
        "#             print(\"subg.edata: \", subg.edata)\n",
        "#             print(\"subg.ndata: \", subg.ndata)\n",
        "#             print(\"subg.srcdata: \", subg.srcdata)\n",
        "#             print(subg.num_nodes(ntype=\"_N\"))\n",
        "#             print(subg.nodes(ntype=\"_N\"))\n",
        "#             print(\"subg.dstdata: \", subg.dstdata)\n",
        "\n",
        "            # {'_ID': {'_N': tensor([0, 1])}, '_orgID': {'_N': tensor([1, 2])}})\n",
        "            # t_dict = {}\n",
        "            # t_dict[\"_N\"] = ts.repeat(subg.num_nodes(ntype=\"_N\"))\n",
        "            subg.ndata['timestamp'] = ts.repeat(subg.num_nodes())\n",
        "            # print(\"subg: \",  subg)\n",
        "            # print(\"subg.ndata: \", subg.ndata)\n",
        "            # print(\"subg.edata: \", subg.edata)\n",
        "            nodes_id.append(subg.srcdata[dgl.NID])\n",
        "            batch_graphs.append(subg)\n",
        "\n",
        "        timestamps = torch.tensor(timestamps).repeat_interleave(self.negative_sampler.k)\n",
        "        # for i, neg_edge in enumerate(zip(neg_srcdst_raw[0].tolist(), neg_srcdst_raw[1].tolist())):\n",
        "        neg_list = []\n",
        "\n",
        "        src, dst = neg_graph.edges()\n",
        "        for i, src_node in enumerate(src):\n",
        "            neg_list.append([neg_graph.ndata[dgl.NID][src_node], neg_graph.ndata[dgl.NID][dst[i]]])\n",
        "        for i, neg_edge in enumerate(neg_list):\n",
        "            ts = timestamps[i]\n",
        "            # print(\"neg_edge: \", neg_edge)\n",
        "            _, _, subgs = self.sampler.sample_blocks_for_Batch(g,\n",
        "                                                    neg_edge,\n",
        "                                                    timestamp=ts)\n",
        "            subg = subgs[0]\n",
        "            # print(\"subg ndata: \", subg.ndata)\n",
        "            # t_dict = {}\n",
        "            # t_dict[\"_N\"] = ts.repeat(subg.num_nodes(ntype=\"_N\"))\n",
        "            # subg.ndata['timestamp'] = t_dict#ts.repeat(subg.num_nodes(ntype=\"_N\"))\n",
        "            subg.ndata['timestamp'] = ts.repeat(subg.num_nodes())\n",
        "            # subg.ndata['timestamp'] = ts.repeat(subg.num_nodes())\n",
        "            batch_graphs.append(subg)\n",
        "        blocks = [dgl.batch(batch_graphs)]\n",
        "        input_nodes = torch.cat(nodes_id)\n",
        "        # return input_nodes, pair_graph, neg_pair_graph, blocks\n",
        "\n",
        "\n",
        "        #time_stamp of the first seed edge\n",
        "        # print(\"seed edges: \", see)\n",
        "        # timestamp =\n",
        "        # input_nodes, _, blocks = self.sampler.sample(g, seed_nodes, exclude_eids, timestamp = timestamps[0])\n",
        "\n",
        "        if self.negative_sampler is None:\n",
        "            return self.assign_lazy_features((input_nodes, pair_graph, blocks))\n",
        "        else:\n",
        "            return self.assign_lazy_features((input_nodes, pair_graph, neg_graph, blocks))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a39bedb7-ed53-4e7c-bc27-5ee26f0976ed",
      "metadata": {
        "id": "a39bedb7-ed53-4e7c-bc27-5ee26f0976ed"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NeighborSampler(Sampler):\n",
        "    def __init__(self, fanouts):\n",
        "        super().__init__()\n",
        "        self.fanouts = fanouts\n",
        "\n",
        "    # NOTE: There is an additional third argument. For homogeneous graphs,\n",
        "    #   it is an 1-D tensor of integer IDs. For heterogeneous graphs, it\n",
        "    #   is a dictionary of ID tensors. We usually set its default value to be None.\n",
        "    def sample(self, g, seed_nodes, exclude_eids=None):\n",
        "        output_nodes = seed_nodes\n",
        "        subgs = []\n",
        "        for fanout in reversed(self.fanouts):\n",
        "            # Sample a fixed number of neighbors of the current seed nodes.\n",
        "            sg = g.sample_neighbors(seed_nodes, fanout, exclude_edges=exclude_eids)\n",
        "            # Convert this subgraph to a message flow graph.\n",
        "            sg = dgl.to_block(sg, seed_nodes)\n",
        "            seed_nodes = sg.srcdata[NID]\n",
        "            subgs.insert(0, sg)\n",
        "        input_nodes = seed_nodes\n",
        "        return input_nodes, output_nodes, subgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c33ae43-4abf-4264-8ab4-f0d9bbdff6c8",
      "metadata": {
        "id": "9c33ae43-4abf-4264-8ab4-f0d9bbdff6c8"
      },
      "outputs": [],
      "source": [
        "def _get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda'\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    device = torch.device(device)\n",
        "    if device.type == 'cuda' and device.index is None:\n",
        "        device = torch.device('cuda', torch.cuda.current_device())\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1100555f-8e01-49d2-97cf-7959af16846d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1100555f-8e01-49d2-97cf-7959af16846d",
        "outputId": "7de8b84d-50d3-4e61-9611-5a081a4e11c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = _get_device()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9afc2a3-8e60-4739-8c61-fbb3a4a9f74f",
      "metadata": {
        "id": "a9afc2a3-8e60-4739-8c61-fbb3a4a9f74f"
      },
      "outputs": [],
      "source": [
        "temporal_sampler = TemporalSampler(k=args.n_neighbors)\n",
        "neg_sampler = dgl.dataloading.negative_sampler.Uniform(k=args.num_negative_samples)\n",
        "temporal_edge_sampler = BatchedTemporalEdgePredictionSampler(temporal_sampler,  negative_sampler=neg_sampler)\n",
        "# sampler = NeighborSampler(args.n_neighbors)\n",
        "# edge_collator = TemporalEdgeCollator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960b5396-e838-405a-87e0-9f3acd539a91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "960b5396-e838-405a-87e0-9f3acd539a91",
        "outputId": "5f65c5d7-62c5-409f-e2c8-7964f287d753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 1, 2, 1, 2, 1, 4]) tensor([8227, 8228, 8228, 8229, 8228, 8229, 8228, 8231])\n",
            "tensor([  0.,  36.,  77., 131., 150., 153., 217., 218.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "src, dst = graph_no_new_node.edges()\n",
        "print(src[0:8], dst[0:8])\n",
        "print(graph_no_new_node.edata['timestamp'][0:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd8558f-179a-4c39-bf08-0c278b984e2f",
      "metadata": {
        "id": "0dd8558f-179a-4c39-bf08-0c278b984e2f"
      },
      "outputs": [],
      "source": [
        "# # sampler = dgl.dataloading.MultiLayerNeighborSampler([15, 10, 5])\n",
        "# dataloader = dgl.dataloading.DataLoader(\n",
        "#     graph_no_new_node, train_seed, temporal_edge_sampler,\n",
        "#     batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=0, device=device)#collate_fn = edge_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0d37bd-6eaa-4229-aaec-a54c59a268d0",
      "metadata": {
        "id": "4b0d37bd-6eaa-4229-aaec-a54c59a268d0"
      },
      "outputs": [],
      "source": [
        "# sampler = dgl.dataloading.MultiLayerNeighborSampler([15, 10, 5])\n",
        "train_dataloader = dgl.dataloading.DataLoader(\n",
        "    graph_no_new_node, train_seed, temporal_edge_sampler,\n",
        "    batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=0, device=device)#collate_fn = edge_collator\n",
        "\n",
        "# sampler = dgl.dataloading.MultiLayerNeighborSampler([15, 10, 5])\n",
        "valid_dataloader = dgl.dataloading.DataLoader(\n",
        "    graph_no_new_node, valid_seed, temporal_edge_sampler,\n",
        "    batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=0, device=device)#collate_fn = edge_collator\n",
        "\n",
        "test_new_node_dataloader = dgl.dataloading.DataLoader(\n",
        "    graph_new_node,test_new_node_seed, temporal_edge_sampler,\n",
        "    batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=0, device=device)#collate_fn = edge_collator\n",
        "\n",
        "test_dataloader = dgl.dataloading.DataLoader(\n",
        "    graph_no_new_node, test_seed, temporal_edge_sampler,\n",
        "    batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=0, device=device)#collate_fn = edge_collator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7713c4b7-7365-4f1f-bc3d-ee914d434763",
      "metadata": {
        "id": "7713c4b7-7365-4f1f-bc3d-ee914d434763"
      },
      "outputs": [],
      "source": [
        "# with dataloader.enable_cpu_affinity():\n",
        "# input_nodes, pair_graph, neg_graph, blocks = example_minibatch = next(\n",
        "#     iter(dataloader)\n",
        "# )\n",
        "# with dataloader.enable_cpu_affinity():\n",
        "#     input_nodes, pair_graph, neg_graph, blocks = example_minibatch = next(\n",
        "#         iter(dataloader)\n",
        "#     )\n",
        "# # print(\"mini_batch: \", example_minibatch)\n",
        "# print(\"input_nodes: \", input_nodes)\n",
        "# print(\"pair graph: \", pair_graph)\n",
        "# print(\"Pair Graph NID: \", pair_graph.ndata[dgl.NID])\n",
        "# print(\"Pair Graph EID: \", pair_graph.edata[dgl.EID])\n",
        "# # print(neg_graph.ndata[dgl.NID])\n",
        "# # print(neg_graph.edata[dgl.EID])\n",
        "# print(blocks[0].srcdata[dgl.NID])\n",
        "# print(blocks[0].dstdata[dgl.NID])\n",
        "# print(blocks[0].ndata[dgl.NID])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7d001a3-f8f8-4907-8cb0-01f6ea65c6e9",
      "metadata": {
        "id": "e7d001a3-f8f8-4907-8cb0-01f6ea65c6e9"
      },
      "outputs": [],
      "source": [
        "# for _, positive_pair_g, negative_pair_g, blocks in dataloader:\n",
        "#     block = blocks[0]\n",
        "#     print(block)\n",
        "#     print(block.edata)\n",
        "#     print(block.ndata)\n",
        "#     print(block.edges())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf006ec-ffb9-464c-b580-1973be3a4be0",
      "metadata": {
        "id": "9cf006ec-ffb9-464c-b580-1973be3a4be0"
      },
      "outputs": [],
      "source": [
        "edge_dim = data.edata['feats'].shape[1]\n",
        "num_node = data.num_nodes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28dbea89-5cf7-48b1-9d6f-a5f849ebe92f",
      "metadata": {
        "id": "28dbea89-5cf7-48b1-9d6f-a5f849ebe92f"
      },
      "outputs": [],
      "source": [
        "from modules import MemoryModule, MemoryOperation, MsgLinkPredictor, TemporalTransformerConv, TimeEncode\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8467cffb-4c25-4246-990a-3c2e55bc834c",
      "metadata": {
        "id": "8467cffb-4c25-4246-990a-3c2e55bc834c"
      },
      "outputs": [],
      "source": [
        "class TGN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 edge_feat_dim,\n",
        "                 memory_dim,\n",
        "                 temporal_dim,\n",
        "                 embedding_dim,\n",
        "                 num_heads,\n",
        "                 num_nodes,\n",
        "                 n_neighbors=10,\n",
        "                 memory_updater_type='gru',\n",
        "                 mem_device = None,\n",
        "                 layers=1):\n",
        "        super(TGN, self).__init__()\n",
        "        self.memory_dim = memory_dim\n",
        "        self.edge_feat_dim = edge_feat_dim\n",
        "        self.temporal_dim = temporal_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.memory_updater_type = memory_updater_type\n",
        "        self.num_nodes = num_nodes\n",
        "        self.layers = layers\n",
        "\n",
        "        self.temporal_encoder = TimeEncode(self.temporal_dim)\n",
        "\n",
        "        self.memory = MemoryModule(self.num_nodes,\n",
        "                                   self.memory_dim, mem_device=mem_device)\n",
        "\n",
        "        self.memory_ops = MemoryOperation(self.memory_updater_type,\n",
        "                                          self.memory,\n",
        "                                          self.edge_feat_dim,\n",
        "                                          self.temporal_encoder)\n",
        "\n",
        "        self.embedding_attn = TemporalTransformerConv(self.edge_feat_dim,\n",
        "                                                      self.memory_dim,\n",
        "                                                      self.temporal_encoder,\n",
        "                                                      self.embedding_dim,\n",
        "                                                      self.num_heads,\n",
        "                                                      layers=self.layers,\n",
        "                                                      allow_zero_in_degree=True)\n",
        "\n",
        "        self.msg_linkpredictor = MsgLinkPredictor(embedding_dim)\n",
        "\n",
        "    def embed(self, postive_graph, negative_graph, blocks):\n",
        "        emb_graph = blocks[0]\n",
        "        # nids = emb_graph.ndata[\"_orgID\"][\"_N\"]\n",
        "\n",
        "        # print(\"emb_graph ndata: \", emb_graph.ndata)\n",
        "        # print(\"postive graph ndata: \", postive_graph.ndata)\n",
        "        # print(\"negative graph ndata: \", negative_graph.ndata)\n",
        "        # print(\"_ID of nodes: \", emb_graph.ndata[dgl.NID])\n",
        "        # print(\"_orgID of nodes: \", nids)\n",
        "        emb_memory = self.memory.memory[emb_graph.ndata[dgl.NID], :]\n",
        "        # emb_memory = self.memory.memory[emb_graph.ndata[dgl.NID], :]\n",
        "        # emb_memory = self.memory.memory[nids, :]\n",
        "        emb_t = emb_graph.ndata['timestamp']\n",
        "        embedding = self.embedding_attn(emb_graph, emb_memory, emb_t)\n",
        "        # emb2pred = dict(\n",
        "        #     zip(emb_graph.ndata[dgl.NID].tolist(), emb_graph.nodes().tolist()))\n",
        "        emb2pred = dict(\n",
        "            zip(emb_graph.ndata[dgl.NID].tolist(), emb_graph.nodes().tolist()))\n",
        "        # Since postive graph and negative graph has same is mapping\n",
        "        # feat_id = [emb2pred[int(n)] for n in postive_graph.ndata[dgl.NID]]\n",
        "        feat_id = [emb2pred[int(n)] for n in postive_graph.ndata[dgl.NID]]\n",
        "        feat = embedding[feat_id]\n",
        "        pred_pos, pred_neg = self.msg_linkpredictor(\n",
        "            feat, postive_graph, negative_graph)\n",
        "        return pred_pos, pred_neg\n",
        "\n",
        "    def update_memory(self, subg):\n",
        "        new_g = self.memory_ops(subg)\n",
        "        self.memory.set_memory(new_g.ndata[dgl.NID], new_g.ndata['memory'])\n",
        "        self.memory.set_last_update_t(\n",
        "            new_g.ndata[dgl.NID], new_g.ndata['timestamp'])\n",
        "\n",
        "    # Some memory operation wrappers\n",
        "    def detach_memory(self):\n",
        "        self.memory.detach_memory()\n",
        "\n",
        "    def reset_memory(self):\n",
        "        self.memory.reset_memory()\n",
        "\n",
        "    def store_memory(self):\n",
        "        memory_checkpoint = {}\n",
        "        memory_checkpoint['memory'] = copy.deepcopy(self.memory.memory)\n",
        "        memory_checkpoint['last_t'] = copy.deepcopy(self.memory.last_update_t)\n",
        "        return memory_checkpoint\n",
        "\n",
        "    def restore_memory(self, memory_checkpoint):\n",
        "        self.memory.memory = memory_checkpoint['memory']\n",
        "        self.memory.last_update_time = memory_checkpoint['last_t']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.cuda.amp import autocast, GradScaler\n",
        "# scaler = GradScaler()"
      ],
      "metadata": {
        "id": "NCL_TBJXFeAz"
      },
      "id": "NCL_TBJXFeAz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a3a91e3-6af1-4f3d-b184-daf1d8adc43c",
      "metadata": {
        "id": "3a3a91e3-6af1-4f3d-b184-daf1d8adc43c"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, sampler, criterion, optimizer, args):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batch_cnt = 0\n",
        "    last_t = time.time()\n",
        "    data_loader_stime = time.time()\n",
        "    dl_tt = 0\n",
        "    tr_tt = 0\n",
        "    for _, positive_pair_g, negative_pair_g, blocks in dataloader:\n",
        "        # data_loader_etime = time.time()\n",
        "        dt = time.time()-data_loader_stime\n",
        "        # print(\"Data loader Batch: \", batch_cnt, \"Time: \", dt, \" dl_total: \", dl_tt, \" tr_total: \", tr_tt)\n",
        "        dl_tt = dl_tt + dt\n",
        "        optimizer.zero_grad()\n",
        "        # with autocast():\n",
        "        pred_pos, pred_neg = model.embed(\n",
        "              positive_pair_g, negative_pair_g, blocks)\n",
        "        loss = criterion(pred_pos, torch.ones_like(pred_pos))\n",
        "        loss += criterion(pred_neg, torch.zeros_like(pred_neg))\n",
        "        lval = float(loss)*args.batch_size\n",
        "\n",
        "        total_loss += lval\n",
        "        retain_graph = True if batch_cnt == 0 and not args.fast_mode else False\n",
        "        # scaler.scale(loss).backward(retain_graph=retain_graph)\n",
        "        # scaler.step(optimizer)\n",
        "        # scaler.update()\n",
        "        loss.backward(retain_graph=retain_graph)\n",
        "        optimizer.step()\n",
        "        model.detach_memory()\n",
        "        if not args.not_use_memory:\n",
        "          # with autocast():\n",
        "          model.update_memory(positive_pair_g)\n",
        "            # if args.fast_mode:\n",
        "            #     sampler.attach_last_update(model.memory.last_update_t)\n",
        "\n",
        "        dt = time.time()-last_t\n",
        "        tr_tt  = tr_tt + dt\n",
        "        print(\"Data loader Batch: \", batch_cnt, \"Time: \", dt, \" dl_total: \", dl_tt, \" tr_total: \", tr_tt, \" loss: \", lval)\n",
        "        # print(\"Batch: \", batch_cnt, \"Time: \", dt)\n",
        "\n",
        "        last_t = time.time()\n",
        "        batch_cnt += 1\n",
        "        data_loader_stime = time.time()\n",
        "    return total_loss, dl_tt, tr_tt\n",
        "\n",
        "\n",
        "def test_val(model, dataloader, sampler, criterion, args):\n",
        "    model.eval()\n",
        "    batch_size = args.batch_size\n",
        "    total_loss = 0\n",
        "    aps, aucs = [], []\n",
        "    batch_cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for _, postive_pair_g, negative_pair_g, blocks in dataloader:\n",
        "            pred_pos, pred_neg = model.embed(\n",
        "                postive_pair_g, negative_pair_g, blocks)\n",
        "            loss = criterion(pred_pos, torch.ones_like(pred_pos))\n",
        "            loss += criterion(pred_neg, torch.zeros_like(pred_neg))\n",
        "            total_loss += float(loss)*batch_size\n",
        "            y_pred = torch.cat([pred_pos, pred_neg], dim=0).sigmoid().cpu()\n",
        "            y_true = torch.cat(\n",
        "                [torch.ones(pred_pos.size(0)), torch.zeros(pred_neg.size(0))], dim=0)\n",
        "            if not args.not_use_memory:\n",
        "                model.update_memory(postive_pair_g)\n",
        "            if args.fast_mode:\n",
        "                sampler.attach_last_update(model.memory.last_update_t)\n",
        "            is_bad = torch.logical_or(torch.abs(y_pred) > 10, torch.isnan(y_pred))\n",
        "            y_pred[is_bad] = 0\n",
        "            aps.append(average_precision_score(y_true, y_pred))\n",
        "            aucs.append(roc_auc_score(y_true, y_pred))\n",
        "            batch_cnt += 1\n",
        "            # print(\"Batch: \", batch_cnt, \"AP: \", aps[-1], \"AUC: \", aucs[-1])\n",
        "\n",
        "    return float(torch.tensor(aps).mean()), float(torch.tensor(aucs).mean())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fa1aaa-784a-4d9a-a588-20401a66e14b",
      "metadata": {
        "id": "c1fa1aaa-784a-4d9a-a588-20401a66e14b"
      },
      "outputs": [],
      "source": [
        "edge_dim = data.edata['feats'].shape[1]\n",
        "num_node = data.num_nodes()\n",
        "model = TGN(edge_feat_dim=edge_dim,\n",
        "                memory_dim=args.memory_dim,\n",
        "                temporal_dim=args.temporal_dim,\n",
        "                embedding_dim=args.embedding_dim,\n",
        "                num_heads=args.num_heads,\n",
        "                num_nodes=num_node,\n",
        "                n_neighbors=args.n_neighbors,\n",
        "                memory_updater_type=args.memory_updater,\n",
        "                mem_device = device,\n",
        "                layers=args.k_hop)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
        "f = open(\"logging.txt\", 'w')\n",
        "# if args.fast_mode:\n",
        "#     sampler.reset()\n",
        "# model.to(device)\n",
        "model = model.to(device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6d13526-b99d-4c1c-831a-ae19e8e11267",
      "metadata": {
        "id": "d6d13526-b99d-4c1c-831a-ae19e8e11267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f767d267-ac1c-4e13-80c9-85f298db0ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(model.memory.memory)\n",
        "# next(model.memory.memory.parameters()).is_cuda\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "262243bb-bcb3-4b0a-b9ee-561a6fb0d221",
      "metadata": {
        "id": "262243bb-bcb3-4b0a-b9ee-561a6fb0d221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cf712a3f-a3ac-4a90-9e28-42a895d2231f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "At the beginning\n",
            "Data loader Batch:  0 Time:  6.376658916473389  dl_total:  6.228303670883179  tr_total:  6.376658916473389  loss:  1687.0816650390625\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:818: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loader Batch:  1 Time:  6.330328941345215  dl_total:  12.382943153381348  tr_total:  12.706987857818604  loss:  1507.561279296875\n",
            "Data loader Batch:  2 Time:  6.643822908401489  dl_total:  18.822046041488647  tr_total:  19.350810766220093  loss:  1442.23486328125\n",
            "Data loader Batch:  3 Time:  6.173504590988159  dl_total:  24.84015965461731  tr_total:  25.524315357208252  loss:  1487.2637939453125\n",
            "Data loader Batch:  4 Time:  6.240732431411743  dl_total:  30.92544412612915  tr_total:  31.765047788619995  loss:  1664.20263671875\n",
            "Data loader Batch:  5 Time:  6.165156841278076  dl_total:  36.93287110328674  tr_total:  37.93020462989807  loss:  1529.678955078125\n",
            "Data loader Batch:  6 Time:  6.2133190631866455  dl_total:  42.98992562294006  tr_total:  44.14352369308472  loss:  1628.1861572265625\n",
            "Data loader Batch:  7 Time:  6.214906215667725  dl_total:  49.04952335357666  tr_total:  50.35842990875244  loss:  1540.064208984375\n",
            "Data loader Batch:  8 Time:  6.281335115432739  dl_total:  55.166354179382324  tr_total:  56.63976502418518  loss:  1723.104736328125\n",
            "Data loader Batch:  9 Time:  6.252078533172607  dl_total:  61.26510167121887  tr_total:  62.89184355735779  loss:  1525.238037109375\n",
            "Data loader Batch:  10 Time:  6.373971223831177  dl_total:  67.48616552352905  tr_total:  69.26581478118896  loss:  1502.839111328125\n",
            "Data loader Batch:  11 Time:  6.17936897277832  dl_total:  73.51957845687866  tr_total:  75.44518375396729  loss:  1533.4403076171875\n",
            "Data loader Batch:  12 Time:  6.2214977741241455  dl_total:  79.5877058506012  tr_total:  81.66668152809143  loss:  1521.9217529296875\n",
            "Data loader Batch:  13 Time:  6.01567268371582  dl_total:  85.44868445396423  tr_total:  87.68235421180725  loss:  1466.812744140625\n",
            "Data loader Batch:  14 Time:  6.30340313911438  dl_total:  91.59552359580994  tr_total:  93.98575735092163  loss:  1805.486572265625\n",
            "Data loader Batch:  15 Time:  6.198075771331787  dl_total:  97.64178395271301  tr_total:  100.18383312225342  loss:  1603.1416015625\n",
            "Data loader Batch:  16 Time:  6.297988176345825  dl_total:  103.7913236618042  tr_total:  106.48182129859924  loss:  1431.09130859375\n",
            "Data loader Batch:  17 Time:  6.116598129272461  dl_total:  109.75585722923279  tr_total:  112.5984194278717  loss:  1894.41455078125\n",
            "Data loader Batch:  18 Time:  6.263170003890991  dl_total:  115.87394452095032  tr_total:  118.8615894317627  loss:  1550.422607421875\n",
            "Data loader Batch:  19 Time:  6.262871265411377  dl_total:  121.97766351699829  tr_total:  125.12446069717407  loss:  1418.94873046875\n",
            "Data loader Batch:  20 Time:  6.326167345046997  dl_total:  128.13832592964172  tr_total:  131.45062804222107  loss:  1446.043212890625\n",
            "Data loader Batch:  21 Time:  6.232213258743286  dl_total:  134.2162058353424  tr_total:  137.68284130096436  loss:  1601.4556884765625\n",
            "Data loader Batch:  22 Time:  6.095029830932617  dl_total:  140.1530418395996  tr_total:  143.77787113189697  loss:  1411.4267578125\n",
            "Data loader Batch:  23 Time:  6.192301988601685  dl_total:  146.20021152496338  tr_total:  149.97017312049866  loss:  1465.1761474609375\n",
            "Data loader Batch:  24 Time:  6.266618251800537  dl_total:  152.31410241127014  tr_total:  156.2367913722992  loss:  1555.297119140625\n",
            "Data loader Batch:  25 Time:  6.135931968688965  dl_total:  158.3028314113617  tr_total:  162.37272334098816  loss:  1420.69775390625\n",
            "Data loader Batch:  26 Time:  6.259005784988403  dl_total:  164.4042603969574  tr_total:  168.63172912597656  loss:  1420.577392578125\n",
            "Data loader Batch:  27 Time:  6.202394962310791  dl_total:  170.44149565696716  tr_total:  174.83412408828735  loss:  1425.381591796875\n",
            "Data loader Batch:  28 Time:  6.246240139007568  dl_total:  176.53083395957947  tr_total:  181.08036422729492  loss:  1545.6573486328125\n",
            "Data loader Batch:  29 Time:  6.261120080947876  dl_total:  182.64279341697693  tr_total:  187.3414843082428  loss:  1432.998779296875\n",
            "Data loader Batch:  30 Time:  6.247800588607788  dl_total:  188.7361843585968  tr_total:  193.5892848968506  loss:  1428.356689453125\n",
            "Data loader Batch:  31 Time:  6.15829062461853  dl_total:  194.74266839027405  tr_total:  199.74757552146912  loss:  1413.35205078125\n",
            "Data loader Batch:  32 Time:  5.9993979930877686  dl_total:  200.58549976348877  tr_total:  205.74697351455688  loss:  1413.0911865234375\n",
            "Data loader Batch:  33 Time:  6.279872179031372  dl_total:  206.69770407676697  tr_total:  212.02684569358826  loss:  1445.279541015625\n",
            "Data loader Batch:  34 Time:  6.22298526763916  dl_total:  212.76717877388  tr_total:  218.24983096122742  loss:  1400.543701171875\n",
            "Data loader Batch:  35 Time:  6.256377458572388  dl_total:  218.86859560012817  tr_total:  224.5062084197998  loss:  1479.8624267578125\n",
            "Data loader Batch:  36 Time:  6.262517929077148  dl_total:  224.96767926216125  tr_total:  230.76872634887695  loss:  1470.20166015625\n",
            "Data loader Batch:  37 Time:  6.2742531299591064  dl_total:  231.0873670578003  tr_total:  237.04297947883606  loss:  1421.995849609375\n",
            "Data loader Batch:  38 Time:  6.230511426925659  dl_total:  237.1702539920807  tr_total:  243.27349090576172  loss:  1513.052734375\n",
            "Data loader Batch:  39 Time:  6.274140357971191  dl_total:  243.28723812103271  tr_total:  249.5476312637329  loss:  1416.633056640625\n",
            "Data loader Batch:  40 Time:  6.229311227798462  dl_total:  249.3688623905182  tr_total:  255.77694249153137  loss:  1391.6986083984375\n",
            "Data loader Batch:  41 Time:  6.161762475967407  dl_total:  255.37654995918274  tr_total:  261.9387049674988  loss:  1398.55712890625\n",
            "Data loader Batch:  42 Time:  6.233089447021484  dl_total:  261.40174102783203  tr_total:  268.17179441452026  loss:  1408.28466796875\n",
            "Data loader Batch:  43 Time:  6.229800462722778  dl_total:  267.4733216762543  tr_total:  274.40159487724304  loss:  1407.861572265625\n",
            "Data loader Batch:  44 Time:  6.2824952602386475  dl_total:  273.5815246105194  tr_total:  280.6840901374817  loss:  1398.7509765625\n",
            "Data loader Batch:  45 Time:  6.237604379653931  dl_total:  279.65286016464233  tr_total:  286.9216945171356  loss:  1560.064697265625\n",
            "Data loader Batch:  46 Time:  6.222020626068115  dl_total:  285.7202045917511  tr_total:  293.14371514320374  loss:  1393.902587890625\n",
            "Data loader Batch:  47 Time:  6.244127035140991  dl_total:  291.81198596954346  tr_total:  299.3878421783447  loss:  1421.4478759765625\n",
            "Data loader Batch:  48 Time:  6.1376097202301025  dl_total:  297.7946255207062  tr_total:  305.52545189857483  loss:  1420.93115234375\n",
            "Data loader Batch:  49 Time:  6.2737791538238525  dl_total:  303.9176151752472  tr_total:  311.7992310523987  loss:  1471.602294921875\n",
            "Data loader Batch:  50 Time:  6.218645334243774  dl_total:  309.9822220802307  tr_total:  318.01787638664246  loss:  1401.6861572265625\n",
            "Data loader Batch:  51 Time:  6.312698841094971  dl_total:  316.14437317848206  tr_total:  324.3305752277374  loss:  1442.6627197265625\n",
            "Data loader Batch:  52 Time:  6.266585111618042  dl_total:  322.26097202301025  tr_total:  330.59716033935547  loss:  1370.79541015625\n",
            "Data loader Batch:  53 Time:  6.351984262466431  dl_total:  328.4534866809845  tr_total:  336.9491446018219  loss:  1440.0118408203125\n",
            "Data loader Batch:  54 Time:  6.058632135391235  dl_total:  334.36297702789307  tr_total:  343.00777673721313  loss:  3039.201416015625\n",
            "Data loader Batch:  55 Time:  6.3452675342559814  dl_total:  340.541127204895  tr_total:  349.3530442714691  loss:  1408.6171875\n",
            "Data loader Batch:  56 Time:  6.221145153045654  dl_total:  346.60596799850464  tr_total:  355.57418942451477  loss:  1413.80712890625\n",
            "Data loader Batch:  57 Time:  6.325331926345825  dl_total:  352.7728428840637  tr_total:  361.8995213508606  loss:  1410.4158935546875\n",
            "Data loader Batch:  58 Time:  6.182946681976318  dl_total:  358.80470299720764  tr_total:  368.0824680328369  loss:  1444.56689453125\n",
            "Data loader Batch:  59 Time:  6.3481340408325195  dl_total:  364.9961757659912  tr_total:  374.43060207366943  loss:  1405.03759765625\n",
            "Data loader Batch:  60 Time:  6.223985195159912  dl_total:  371.0711443424225  tr_total:  380.65458726882935  loss:  1361.176513671875\n",
            "Data loader Batch:  61 Time:  6.250350713729858  dl_total:  377.1735100746155  tr_total:  386.9049379825592  loss:  1411.63720703125\n",
            "Data loader Batch:  62 Time:  6.196357250213623  dl_total:  383.2148084640503  tr_total:  393.1012952327728  loss:  1484.433837890625\n",
            "Data loader Batch:  63 Time:  6.376324415206909  dl_total:  389.4410169124603  tr_total:  399.47761964797974  loss:  1504.478515625\n",
            "Data loader Batch:  64 Time:  6.3451714515686035  dl_total:  395.6299524307251  tr_total:  405.82279109954834  loss:  1464.2763671875\n",
            "Data loader Batch:  65 Time:  6.503098726272583  dl_total:  401.97179794311523  tr_total:  412.3258898258209  loss:  1416.2978515625\n",
            "Data loader Batch:  66 Time:  6.297004461288452  dl_total:  408.11672353744507  tr_total:  418.6228942871094  loss:  1437.586181640625\n",
            "Data loader Batch:  67 Time:  6.350879907608032  dl_total:  414.31176590919495  tr_total:  424.9737741947174  loss:  1443.170166015625\n",
            "Data loader Batch:  68 Time:  6.290008783340454  dl_total:  420.43502402305603  tr_total:  431.26378297805786  loss:  1434.563232421875\n",
            "Data loader Batch:  69 Time:  6.216768980026245  dl_total:  426.4850342273712  tr_total:  437.4805519580841  loss:  1415.78125\n",
            "Data loader Batch:  70 Time:  6.303303241729736  dl_total:  432.63798093795776  tr_total:  443.78385519981384  loss:  1402.1878662109375\n",
            "Data loader Batch:  71 Time:  6.318486452102661  dl_total:  438.80396723747253  tr_total:  450.1023416519165  loss:  1399.95166015625\n",
            "Data loader Batch:  72 Time:  6.201576471328735  dl_total:  444.8542788028717  tr_total:  456.30391812324524  loss:  1423.73486328125\n",
            "Data loader Batch:  73 Time:  6.3033833503723145  dl_total:  451.0022530555725  tr_total:  462.60730147361755  loss:  1455.8038330078125\n",
            "Data loader Batch:  74 Time:  6.328845024108887  dl_total:  457.17681193351746  tr_total:  468.93614649772644  loss:  1391.9769287109375\n",
            "Data loader Batch:  75 Time:  6.384743690490723  dl_total:  463.38686752319336  tr_total:  475.32089018821716  loss:  1393.1138916015625\n",
            "Data loader Batch:  76 Time:  6.353659629821777  dl_total:  469.5527882575989  tr_total:  481.67454981803894  loss:  1416.91357421875\n",
            "Data loader Batch:  77 Time:  6.257327556610107  dl_total:  475.6490070819855  tr_total:  487.93187737464905  loss:  1416.4345703125\n",
            "Data loader Batch:  78 Time:  6.3848302364349365  dl_total:  481.88590025901794  tr_total:  494.316707611084  loss:  1429.974853515625\n",
            "Data loader Batch:  79 Time:  6.381454706192017  dl_total:  488.11604595184326  tr_total:  500.698162317276  loss:  1395.770263671875\n",
            "Data loader Batch:  80 Time:  6.48861837387085  dl_total:  494.44158601760864  tr_total:  507.18678069114685  loss:  1462.5908203125\n",
            "Data loader Batch:  81 Time:  6.314245939254761  dl_total:  500.6038131713867  tr_total:  513.5010266304016  loss:  1401.5537109375\n",
            "Data loader Batch:  82 Time:  6.121642827987671  dl_total:  506.56777572631836  tr_total:  519.6226694583893  loss:  1429.87451171875\n",
            "Data loader Batch:  83 Time:  6.216733932495117  dl_total:  512.6295213699341  tr_total:  525.8394033908844  loss:  1494.70556640625\n",
            "Data loader Batch:  84 Time:  6.294334650039673  dl_total:  518.7725412845612  tr_total:  532.1337380409241  loss:  1511.98388671875\n",
            "Data loader Batch:  85 Time:  6.3033764362335205  dl_total:  524.9307715892792  tr_total:  538.4371144771576  loss:  1392.482421875\n",
            "Data loader Batch:  86 Time:  6.375843524932861  dl_total:  531.1130836009979  tr_total:  544.8129580020905  loss:  1433.3084716796875\n",
            "Data loader Batch:  87 Time:  6.226248025894165  dl_total:  537.1873862743378  tr_total:  551.0392060279846  loss:  1422.210205078125\n",
            "Data loader Batch:  88 Time:  6.361772298812866  dl_total:  543.3899292945862  tr_total:  557.4009783267975  loss:  1411.3414306640625\n",
            "Data loader Batch:  89 Time:  6.392002105712891  dl_total:  549.6295764446259  tr_total:  563.7929804325104  loss:  1579.975341796875\n",
            "Data loader Batch:  90 Time:  6.418513298034668  dl_total:  555.895788192749  tr_total:  570.211493730545  loss:  1403.146240234375\n",
            "Data loader Batch:  91 Time:  4.920892000198364  dl_total:  560.7455837726593  tr_total:  575.1323857307434  loss:  1431.072021484375\n",
            "Epoch: 0; Training Loss: 136513.62817382812 | Validation AP: 0.872 AUC: 0.858\n",
            " Epoch: 0; Test AP: 0.880 AUC: 0.857\n",
            " Epoch: 0; Test New Node AP: 0.877 AUC: 0.852\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:818: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loader Batch:  0 Time:  6.260049343109131  dl_total:  6.116761207580566  tr_total:  6.260049343109131  loss:  1970.0810546875\n",
            "Data loader Batch:  1 Time:  5.9008800983428955  dl_total:  11.868306636810303  tr_total:  12.160929441452026  loss:  1502.07177734375\n",
            "Data loader Batch:  2 Time:  6.278243064880371  dl_total:  17.99577498435974  tr_total:  18.439172506332397  loss:  1483.22314453125\n",
            "Data loader Batch:  3 Time:  6.203287124633789  dl_total:  24.04429602622986  tr_total:  24.642459630966187  loss:  1456.50927734375\n",
            "Data loader Batch:  4 Time:  6.299283742904663  dl_total:  30.151549339294434  tr_total:  30.94174337387085  loss:  1526.59228515625\n",
            "Data loader Batch:  5 Time:  6.225048542022705  dl_total:  36.18826103210449  tr_total:  37.166791915893555  loss:  1460.021240234375\n",
            "Data loader Batch:  6 Time:  6.250863552093506  dl_total:  42.275803327560425  tr_total:  43.41765546798706  loss:  1550.279541015625\n",
            "Data loader Batch:  7 Time:  6.1586503982543945  dl_total:  48.27137494087219  tr_total:  49.576305866241455  loss:  1542.908203125\n",
            "Data loader Batch:  8 Time:  6.090273141860962  dl_total:  54.18638014793396  tr_total:  55.66657900810242  loss:  1474.8515625\n",
            "Data loader Batch:  9 Time:  6.253662347793579  dl_total:  60.233689308166504  tr_total:  61.920241355895996  loss:  1470.850830078125\n",
            "Data loader Batch:  10 Time:  6.319571256637573  dl_total:  66.39835047721863  tr_total:  68.23981261253357  loss:  1431.7642822265625\n",
            "Data loader Batch:  11 Time:  6.1473283767700195  dl_total:  72.39412498474121  tr_total:  74.38714098930359  loss:  1465.514404296875\n",
            "Data loader Batch:  12 Time:  6.223658561706543  dl_total:  78.46356701850891  tr_total:  80.61079955101013  loss:  1408.3387451171875\n",
            "Data loader Batch:  13 Time:  6.13905143737793  dl_total:  84.44718146324158  tr_total:  86.74985098838806  loss:  1428.2271728515625\n",
            "Data loader Batch:  14 Time:  6.286409139633179  dl_total:  90.56505918502808  tr_total:  93.03626012802124  loss:  1494.114990234375\n",
            "Data loader Batch:  15 Time:  6.219095945358276  dl_total:  96.56605434417725  tr_total:  99.25535607337952  loss:  1576.477294921875\n",
            "Data loader Batch:  16 Time:  6.305613994598389  dl_total:  102.68469262123108  tr_total:  105.5609700679779  loss:  1468.5738525390625\n",
            "Data loader Batch:  17 Time:  6.1893393993377686  dl_total:  108.70022368431091  tr_total:  111.75030946731567  loss:  1567.3447265625\n",
            "Data loader Batch:  18 Time:  6.289166688919067  dl_total:  114.84461784362793  tr_total:  118.03947615623474  loss:  1451.3587646484375\n",
            "Data loader Batch:  19 Time:  6.030070543289185  dl_total:  120.72339200973511  tr_total:  124.06954669952393  loss:  1470.73828125\n",
            "Data loader Batch:  20 Time:  6.234555721282959  dl_total:  126.8073034286499  tr_total:  130.30410242080688  loss:  1457.55908203125\n",
            "Data loader Batch:  21 Time:  6.1560797691345215  dl_total:  132.81572222709656  tr_total:  136.4601821899414  loss:  1582.6953125\n",
            "Data loader Batch:  22 Time:  6.252231597900391  dl_total:  138.9150528907776  tr_total:  142.7124137878418  loss:  1432.663818359375\n",
            "Data loader Batch:  23 Time:  6.22573184967041  dl_total:  144.98358249664307  tr_total:  148.9381456375122  loss:  1407.705810546875\n",
            "Data loader Batch:  24 Time:  6.256263732910156  dl_total:  151.08386492729187  tr_total:  155.19440937042236  loss:  1525.0101318359375\n",
            "Data loader Batch:  25 Time:  6.236651182174683  dl_total:  157.1572778224945  tr_total:  161.43106055259705  loss:  1446.678955078125\n",
            "Data loader Batch:  26 Time:  6.3208534717559814  dl_total:  163.32825875282288  tr_total:  167.75191402435303  loss:  1399.500244140625\n",
            "Data loader Batch:  27 Time:  5.94584584236145  dl_total:  169.09818649291992  tr_total:  173.69775986671448  loss:  1407.8665771484375\n",
            "Data loader Batch:  28 Time:  6.281169652938843  dl_total:  175.22306752204895  tr_total:  179.97892951965332  loss:  1409.665771484375\n",
            "Data loader Batch:  29 Time:  6.187576532363892  dl_total:  181.25346183776855  tr_total:  186.1665060520172  loss:  1420.153076171875\n",
            "Data loader Batch:  30 Time:  6.204555511474609  dl_total:  187.30960035324097  tr_total:  192.37106156349182  loss:  1462.766845703125\n",
            "Data loader Batch:  31 Time:  6.18961238861084  dl_total:  193.30530190467834  tr_total:  198.56067395210266  loss:  1381.49853515625\n",
            "Data loader Batch:  32 Time:  6.255180358886719  dl_total:  199.4071569442749  tr_total:  204.81585431098938  loss:  1434.390869140625\n",
            "Data loader Batch:  33 Time:  6.215042352676392  dl_total:  205.47074151039124  tr_total:  211.03089666366577  loss:  1428.5218505859375\n",
            "Data loader Batch:  34 Time:  6.233042240142822  dl_total:  211.54330444335938  tr_total:  217.2639389038086  loss:  1428.065673828125\n",
            "Data loader Batch:  35 Time:  6.217439889907837  dl_total:  217.60679507255554  tr_total:  223.48137879371643  loss:  1474.76123046875\n",
            "Data loader Batch:  36 Time:  6.326754093170166  dl_total:  223.78152513504028  tr_total:  229.8081328868866  loss:  1384.497802734375\n",
            "Data loader Batch:  37 Time:  6.008178949356079  dl_total:  229.63637948036194  tr_total:  235.81631183624268  loss:  1422.070068359375\n",
            "Data loader Batch:  38 Time:  6.217295169830322  dl_total:  235.68112087249756  tr_total:  242.033607006073  loss:  1394.6025390625\n",
            "Data loader Batch:  39 Time:  6.251783132553101  dl_total:  241.75776505470276  tr_total:  248.2853901386261  loss:  1372.35400390625\n",
            "Data loader Batch:  40 Time:  6.248955249786377  dl_total:  247.84852504730225  tr_total:  254.53434538841248  loss:  1362.462890625\n",
            "Data loader Batch:  41 Time:  6.238060235977173  dl_total:  253.93861722946167  tr_total:  260.77240562438965  loss:  1373.0948486328125\n",
            "Data loader Batch:  42 Time:  6.298438310623169  dl_total:  260.0878095626831  tr_total:  267.0708439350128  loss:  1354.995849609375\n",
            "Data loader Batch:  43 Time:  6.230157136917114  dl_total:  266.15703773498535  tr_total:  273.30100107192993  loss:  1352.76220703125\n",
            "Data loader Batch:  44 Time:  6.211576700210571  dl_total:  272.21195554733276  tr_total:  279.5125777721405  loss:  1392.9619140625\n",
            "Data loader Batch:  45 Time:  6.270165681838989  dl_total:  278.3243577480316  tr_total:  285.7827434539795  loss:  1524.1358642578125\n",
            "Data loader Batch:  46 Time:  6.187077045440674  dl_total:  284.3549234867096  tr_total:  291.96982049942017  loss:  1405.6083984375\n",
            "Data loader Batch:  47 Time:  6.265822887420654  dl_total:  290.45640873908997  tr_total:  298.2356433868408  loss:  1353.763671875\n",
            "Data loader Batch:  48 Time:  6.219397068023682  dl_total:  296.51964354515076  tr_total:  304.4550404548645  loss:  1324.132568359375\n",
            "Data loader Batch:  49 Time:  6.086799621582031  dl_total:  302.4504156112671  tr_total:  310.54184007644653  loss:  1399.446533203125\n",
            "Data loader Batch:  50 Time:  6.176215410232544  dl_total:  308.47214221954346  tr_total:  316.7180554866791  loss:  1345.5975341796875\n",
            "Data loader Batch:  51 Time:  6.318736553192139  dl_total:  314.6298818588257  tr_total:  323.0367920398712  loss:  1308.972900390625\n",
            "Data loader Batch:  52 Time:  6.287822484970093  dl_total:  320.75498604774475  tr_total:  329.3246145248413  loss:  1322.850341796875\n",
            "Data loader Batch:  53 Time:  6.254345655441284  dl_total:  326.85556507110596  tr_total:  335.5789601802826  loss:  1317.427490234375\n",
            "Data loader Batch:  54 Time:  6.198338031768799  dl_total:  332.90815591812134  tr_total:  341.7772982120514  loss:  1948.109130859375\n",
            "Data loader Batch:  55 Time:  6.3631591796875  dl_total:  339.0976688861847  tr_total:  348.1404573917389  loss:  1343.6138916015625\n",
            "Data loader Batch:  56 Time:  6.28693699836731  dl_total:  345.226731300354  tr_total:  354.4273943901062  loss:  1476.313720703125\n",
            "Data loader Batch:  57 Time:  6.360155820846558  dl_total:  351.4265446662903  tr_total:  360.78755021095276  loss:  1327.0098876953125\n",
            "Data loader Batch:  58 Time:  6.28694748878479  dl_total:  357.5583415031433  tr_total:  367.07449769973755  loss:  1374.0618896484375\n",
            "Data loader Batch:  59 Time:  6.418399095535278  dl_total:  363.81393671035767  tr_total:  373.4928967952728  loss:  1338.3675537109375\n",
            "Data loader Batch:  60 Time:  6.04533314704895  dl_total:  369.69772005081177  tr_total:  379.5382299423218  loss:  1341.9541015625\n",
            "Data loader Batch:  61 Time:  6.375452280044556  dl_total:  375.86656880378723  tr_total:  385.91368222236633  loss:  1325.6107177734375\n",
            "Data loader Batch:  62 Time:  6.253045082092285  dl_total:  381.9626455307007  tr_total:  392.1667273044586  loss:  1379.0029296875\n",
            "Data loader Batch:  63 Time:  6.323632001876831  dl_total:  388.134548664093  tr_total:  398.49035930633545  loss:  1360.084228515625\n",
            "Data loader Batch:  64 Time:  6.249396800994873  dl_total:  394.2298514842987  tr_total:  404.7397561073303  loss:  1364.392333984375\n",
            "Data loader Batch:  65 Time:  6.376307964324951  dl_total:  400.45107889175415  tr_total:  411.1160640716553  loss:  1333.0830078125\n",
            "Data loader Batch:  66 Time:  6.23693585395813  dl_total:  406.5354354381561  tr_total:  417.3529999256134  loss:  1371.5791015625\n",
            "Data loader Batch:  67 Time:  6.294265985488892  dl_total:  412.6640434265137  tr_total:  423.6472659111023  loss:  1329.119384765625\n",
            "Data loader Batch:  68 Time:  6.32869815826416  dl_total:  418.82525610923767  tr_total:  429.97596406936646  loss:  1348.168212890625\n",
            "Data loader Batch:  69 Time:  6.392756938934326  dl_total:  425.0742189884186  tr_total:  436.3687210083008  loss:  1339.2725830078125\n",
            "Data loader Batch:  70 Time:  6.054391860961914  dl_total:  430.9720757007599  tr_total:  442.4231128692627  loss:  1321.2333984375\n",
            "Data loader Batch:  71 Time:  6.496126890182495  dl_total:  437.13730096817017  tr_total:  448.9192397594452  loss:  1355.1458740234375\n",
            "Data loader Batch:  72 Time:  5.990869045257568  dl_total:  442.9730751514435  tr_total:  454.91010880470276  loss:  1382.0234375\n",
            "Data loader Batch:  73 Time:  6.217889785766602  dl_total:  449.0439558029175  tr_total:  461.12799859046936  loss:  1333.983154296875\n",
            "Data loader Batch:  74 Time:  6.221790790557861  dl_total:  455.1184539794922  tr_total:  467.3497893810272  loss:  1363.6220703125\n",
            "Data loader Batch:  75 Time:  6.348133563995361  dl_total:  461.3173849582672  tr_total:  473.6979229450226  loss:  1399.77294921875\n",
            "Data loader Batch:  76 Time:  6.250924587249756  dl_total:  467.4144752025604  tr_total:  479.94884753227234  loss:  1377.939697265625\n",
            "Data loader Batch:  77 Time:  6.277036428451538  dl_total:  473.5387351512909  tr_total:  486.2258839607239  loss:  1349.453857421875\n",
            "Data loader Batch:  78 Time:  6.230498313903809  dl_total:  479.60695934295654  tr_total:  492.4563822746277  loss:  1424.1724853515625\n",
            "Data loader Batch:  79 Time:  6.388432025909424  dl_total:  485.80491971969604  tr_total:  498.8448143005371  loss:  1358.4522705078125\n",
            "Data loader Batch:  80 Time:  6.290292739868164  dl_total:  491.9361433982849  tr_total:  505.1351070404053  loss:  1405.31005859375\n",
            "Data loader Batch:  81 Time:  6.2811291217803955  dl_total:  498.0584726333618  tr_total:  511.41623616218567  loss:  1398.9774169921875\n",
            "Data loader Batch:  82 Time:  6.244923830032349  dl_total:  504.14097785949707  tr_total:  517.661159992218  loss:  1446.004638671875\n",
            "Data loader Batch:  83 Time:  6.031837701797485  dl_total:  510.01832342147827  tr_total:  523.6929976940155  loss:  1536.09423828125\n",
            "Data loader Batch:  84 Time:  6.184930086135864  dl_total:  516.0494196414948  tr_total:  529.8779277801514  loss:  1473.131591796875\n",
            "Data loader Batch:  85 Time:  6.29796028137207  dl_total:  522.2012031078339  tr_total:  536.1758880615234  loss:  1395.6162109375\n",
            "Data loader Batch:  86 Time:  6.303187608718872  dl_total:  528.3065595626831  tr_total:  542.4790756702423  loss:  1454.261962890625\n",
            "Data loader Batch:  87 Time:  6.227088689804077  dl_total:  534.385115146637  tr_total:  548.7061643600464  loss:  1437.813720703125\n",
            "Data loader Batch:  88 Time:  6.351763486862183  dl_total:  540.5893321037292  tr_total:  555.0579278469086  loss:  1423.214111328125\n",
            "Data loader Batch:  89 Time:  6.30392050743103  dl_total:  546.7411432266235  tr_total:  561.3618483543396  loss:  1500.7794189453125\n",
            "Data loader Batch:  90 Time:  6.312779903411865  dl_total:  552.8952450752258  tr_total:  567.6746282577515  loss:  1435.80859375\n",
            "Data loader Batch:  91 Time:  5.024036645889282  dl_total:  557.8486094474792  tr_total:  572.6986649036407  loss:  1417.167236328125\n",
            "Epoch: 1; Training Loss: 131300.8037109375 | Validation AP: 0.882 AUC: 0.820\n",
            " Epoch: 1; Test AP: 0.877 AUC: 0.814\n",
            " Epoch: 1; Test New Node AP: 0.873 AUC: 0.809\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:818: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loader Batch:  0 Time:  6.244021654129028  dl_total:  6.102843523025513  tr_total:  6.244021654129028  loss:  1758.8250732421875\n",
            "Data loader Batch:  1 Time:  6.282165765762329  dl_total:  12.049316644668579  tr_total:  12.526187419891357  loss:  1442.7701416015625\n",
            "Data loader Batch:  2 Time:  6.1607019901275635  dl_total:  17.88352918624878  tr_total:  18.68688941001892  loss:  1438.3907470703125\n",
            "Data loader Batch:  3 Time:  5.917953252792358  dl_total:  23.6495680809021  tr_total:  24.60484266281128  loss:  1456.6630859375\n",
            "Data loader Batch:  4 Time:  6.145230054855347  dl_total:  29.643171787261963  tr_total:  30.750072717666626  loss:  1493.844970703125\n",
            "Data loader Batch:  5 Time:  6.154524564743042  dl_total:  35.64248251914978  tr_total:  36.90459728240967  loss:  1418.40234375\n",
            "Data loader Batch:  6 Time:  6.175535678863525  dl_total:  41.66331696510315  tr_total:  43.08013296127319  loss:  1433.293701171875\n",
            "Data loader Batch:  7 Time:  6.113873481750488  dl_total:  47.612486362457275  tr_total:  49.19400644302368  loss:  1427.500244140625\n",
            "Data loader Batch:  8 Time:  6.139356851577759  dl_total:  53.59829783439636  tr_total:  55.33336329460144  loss:  1412.16455078125\n",
            "Data loader Batch:  9 Time:  6.181045770645142  dl_total:  59.62580919265747  tr_total:  61.51440906524658  loss:  1406.4844970703125\n",
            "Data loader Batch:  10 Time:  6.293367385864258  dl_total:  65.7657380104065  tr_total:  67.80777645111084  loss:  1369.4217529296875\n",
            "Data loader Batch:  11 Time:  6.24824070930481  dl_total:  71.86541152000427  tr_total:  74.05601716041565  loss:  1437.85498046875\n",
            "Data loader Batch:  12 Time:  6.213137865066528  dl_total:  77.92585873603821  tr_total:  80.26915502548218  loss:  1469.0439453125\n",
            "Data loader Batch:  13 Time:  6.154480218887329  dl_total:  83.92556524276733  tr_total:  86.4236352443695  loss:  1448.620361328125\n",
            "Data loader Batch:  14 Time:  6.24411153793335  dl_total:  90.01494264602661  tr_total:  92.66774678230286  loss:  1474.7060546875\n",
            "Data loader Batch:  15 Time:  6.23647403717041  dl_total:  96.09212446212769  tr_total:  98.90422081947327  loss:  1482.423828125\n",
            "Data loader Batch:  16 Time:  6.274073362350464  dl_total:  102.21347999572754  tr_total:  105.17829418182373  loss:  1417.623046875\n",
            "Data loader Batch:  17 Time:  6.146179914474487  dl_total:  108.20781922340393  tr_total:  111.32447409629822  loss:  1488.19287109375\n",
            "Data loader Batch:  18 Time:  6.259376525878906  dl_total:  114.3211441040039  tr_total:  117.58385062217712  loss:  1455.593017578125\n",
            "Data loader Batch:  19 Time:  5.941151142120361  dl_total:  120.11677289009094  tr_total:  123.52500176429749  loss:  1562.595703125\n",
            "Data loader Batch:  20 Time:  6.28434157371521  dl_total:  126.25374484062195  tr_total:  129.8093433380127  loss:  1403.460693359375\n",
            "Data loader Batch:  21 Time:  6.220115661621094  dl_total:  132.32435011863708  tr_total:  136.0294589996338  loss:  1493.08447265625\n",
            "Data loader Batch:  22 Time:  6.225114345550537  dl_total:  138.40336632728577  tr_total:  142.25457334518433  loss:  1424.263916015625\n",
            "Data loader Batch:  23 Time:  6.119115352630615  dl_total:  144.35946035385132  tr_total:  148.37368869781494  loss:  1424.0513916015625\n",
            "Data loader Batch:  24 Time:  6.287998676300049  dl_total:  150.48167514801025  tr_total:  154.661687374115  loss:  1429.100341796875\n",
            "Data loader Batch:  25 Time:  6.149152755737305  dl_total:  156.47034096717834  tr_total:  160.8108401298523  loss:  1397.8502197265625\n",
            "Data loader Batch:  26 Time:  6.241902828216553  dl_total:  162.5635221004486  tr_total:  167.05274295806885  loss:  1381.5423583984375\n",
            "Data loader Batch:  27 Time:  6.205729722976685  dl_total:  168.60743737220764  tr_total:  173.25847268104553  loss:  1392.966064453125\n",
            "Data loader Batch:  28 Time:  6.006129741668701  dl_total:  174.43916940689087  tr_total:  179.26460242271423  loss:  1413.9998779296875\n",
            "Data loader Batch:  29 Time:  6.185119390487671  dl_total:  180.46336340904236  tr_total:  185.4497218132019  loss:  1429.13623046875\n",
            "Data loader Batch:  30 Time:  6.214120149612427  dl_total:  186.51870250701904  tr_total:  191.66384196281433  loss:  1410.572998046875\n",
            "Data loader Batch:  31 Time:  6.100525379180908  dl_total:  192.47153329849243  tr_total:  197.76436734199524  loss:  1381.47607421875\n",
            "Data loader Batch:  32 Time:  6.193093538284302  dl_total:  198.50862550735474  tr_total:  203.95746088027954  loss:  1353.5802001953125\n",
            "Data loader Batch:  33 Time:  6.264057636260986  dl_total:  204.62160348892212  tr_total:  210.22151851654053  loss:  1411.83203125\n",
            "Data loader Batch:  34 Time:  6.156057596206665  dl_total:  210.4452338218689  tr_total:  216.3775761127472  loss:  1364.938232421875\n",
            "Data loader Batch:  35 Time:  5.980055332183838  dl_total:  216.27232003211975  tr_total:  222.35763144493103  loss:  1389.76806640625\n",
            "Data loader Batch:  36 Time:  6.1746814250946045  dl_total:  222.28349232673645  tr_total:  228.53231287002563  loss:  1370.631591796875\n",
            "Data loader Batch:  37 Time:  6.222958326339722  dl_total:  228.34567761421204  tr_total:  234.75527119636536  loss:  1361.773193359375\n",
            "Data loader Batch:  38 Time:  6.20984673500061  dl_total:  234.40306210517883  tr_total:  240.96511793136597  loss:  1385.771728515625\n",
            "Data loader Batch:  39 Time:  6.211650371551514  dl_total:  240.44332480430603  tr_total:  247.17676830291748  loss:  1301.134521484375\n",
            "Data loader Batch:  40 Time:  6.310368776321411  dl_total:  246.604736328125  tr_total:  253.4871370792389  loss:  1302.096435546875\n",
            "Data loader Batch:  41 Time:  6.326504468917847  dl_total:  252.76003885269165  tr_total:  259.81364154815674  loss:  1317.2880859375\n",
            "Data loader Batch:  42 Time:  6.227208137512207  dl_total:  258.82553243637085  tr_total:  266.04084968566895  loss:  1309.6734619140625\n",
            "Data loader Batch:  43 Time:  6.264665126800537  dl_total:  264.9464981555939  tr_total:  272.3055148124695  loss:  1273.34423828125\n",
            "Data loader Batch:  44 Time:  6.264500617980957  dl_total:  271.0558457374573  tr_total:  278.57001543045044  loss:  1289.302978515625\n",
            "Data loader Batch:  45 Time:  6.3013317584991455  dl_total:  277.20053815841675  tr_total:  284.8713471889496  loss:  1281.983154296875\n",
            "Data loader Batch:  46 Time:  6.322622776031494  dl_total:  283.36911487579346  tr_total:  291.1939699649811  loss:  1317.2398681640625\n",
            "Data loader Batch:  47 Time:  6.312550783157349  dl_total:  289.5234227180481  tr_total:  297.5065207481384  loss:  1291.6728515625\n",
            "Data loader Batch:  48 Time:  5.965145826339722  dl_total:  295.32967019081116  tr_total:  303.47166657447815  loss:  1287.124267578125\n",
            "Data loader Batch:  49 Time:  6.216075420379639  dl_total:  301.3962993621826  tr_total:  309.6877419948578  loss:  1298.134521484375\n",
            "Data loader Batch:  50 Time:  6.204799175262451  dl_total:  307.43396186828613  tr_total:  315.89254117012024  loss:  1278.0380859375\n",
            "Data loader Batch:  51 Time:  6.277590751647949  dl_total:  313.5581295490265  tr_total:  322.1701319217682  loss:  1286.732177734375\n",
            "Data loader Batch:  52 Time:  6.26242733001709  dl_total:  319.67021894454956  tr_total:  328.4325592517853  loss:  1250.00830078125\n",
            "Data loader Batch:  53 Time:  6.3392839431762695  dl_total:  325.8413259983063  tr_total:  334.77184319496155  loss:  1275.057861328125\n",
            "Data loader Batch:  54 Time:  6.32716178894043  dl_total:  332.00708413124084  tr_total:  341.099004983902  loss:  1626.796630859375\n",
            "Data loader Batch:  55 Time:  6.347715377807617  dl_total:  338.18871688842773  tr_total:  347.4467203617096  loss:  1292.71337890625\n",
            "Data loader Batch:  56 Time:  6.015747785568237  dl_total:  344.049782037735  tr_total:  353.46246814727783  loss:  1268.473388671875\n",
            "Data loader Batch:  57 Time:  6.30492377281189  dl_total:  350.19337224960327  tr_total:  359.7673919200897  loss:  1286.9400634765625\n",
            "Data loader Batch:  58 Time:  6.324661493301392  dl_total:  356.32217049598694  tr_total:  366.0920534133911  loss:  1290.014404296875\n",
            "Data loader Batch:  59 Time:  6.390695810317993  dl_total:  362.48924446105957  tr_total:  372.4827492237091  loss:  1233.508056640625\n",
            "Data loader Batch:  60 Time:  6.247360467910767  dl_total:  368.55503606796265  tr_total:  378.7301096916199  loss:  1274.518310546875\n",
            "Data loader Batch:  61 Time:  6.301915884017944  dl_total:  374.70946979522705  tr_total:  385.0320255756378  loss:  1280.8101806640625\n",
            "Data loader Batch:  62 Time:  6.256080150604248  dl_total:  380.7986307144165  tr_total:  391.28810572624207  loss:  1332.478271484375\n",
            "Data loader Batch:  63 Time:  6.304794788360596  dl_total:  386.9513475894928  tr_total:  397.59290051460266  loss:  1417.118896484375\n",
            "Data loader Batch:  64 Time:  6.108189344406128  dl_total:  392.91061878204346  tr_total:  403.7010898590088  loss:  1304.2720947265625\n",
            "Data loader Batch:  65 Time:  6.376383304595947  dl_total:  399.13053798675537  tr_total:  410.07747316360474  loss:  1269.664794921875\n",
            "Data loader Batch:  66 Time:  6.270121335983276  dl_total:  405.25138545036316  tr_total:  416.347594499588  loss:  1310.427978515625\n",
            "Data loader Batch:  67 Time:  6.2684080600738525  dl_total:  411.3664176464081  tr_total:  422.61600255966187  loss:  1313.3416748046875\n",
            "Data loader Batch:  68 Time:  6.352691650390625  dl_total:  417.5586841106415  tr_total:  428.9686942100525  loss:  1293.561279296875\n",
            "Data loader Batch:  69 Time:  6.282909154891968  dl_total:  423.68847274780273  tr_total:  435.25160336494446  loss:  1345.58984375\n",
            "Data loader Batch:  70 Time:  6.357165336608887  dl_total:  429.8880453109741  tr_total:  441.60876870155334  loss:  1282.5230712890625\n",
            "Data loader Batch:  71 Time:  6.271823167800903  dl_total:  436.0032994747162  tr_total:  447.88059186935425  loss:  1397.2802734375\n",
            "Data loader Batch:  72 Time:  6.262218713760376  dl_total:  442.1001527309418  tr_total:  454.1428105831146  loss:  1331.9176025390625\n",
            "Data loader Batch:  73 Time:  6.187715530395508  dl_total:  448.1326677799225  tr_total:  460.33052611351013  loss:  1297.433349609375\n",
            "Data loader Batch:  74 Time:  6.254995822906494  dl_total:  454.2375271320343  tr_total:  466.5855219364166  loss:  1364.2681884765625\n",
            "Data loader Batch:  75 Time:  6.239168882369995  dl_total:  460.3151068687439  tr_total:  472.8246908187866  loss:  1393.19921875\n",
            "Data loader Batch:  76 Time:  6.0614845752716064  dl_total:  466.21609258651733  tr_total:  478.8861753940582  loss:  1420.885009765625\n",
            "Data loader Batch:  77 Time:  6.182170391082764  dl_total:  472.22889494895935  tr_total:  485.068345785141  loss:  1359.0010986328125\n",
            "Data loader Batch:  78 Time:  6.2829201221466064  dl_total:  478.3636393547058  tr_total:  491.3512659072876  loss:  1443.81396484375\n",
            "Data loader Batch:  79 Time:  6.278854846954346  dl_total:  484.46791887283325  tr_total:  497.63012075424194  loss:  1361.451171875\n",
            "Data loader Batch:  80 Time:  6.3409600257873535  dl_total:  490.6548390388489  tr_total:  503.9710807800293  loss:  1385.2900390625\n",
            "Data loader Batch:  81 Time:  6.203253269195557  dl_total:  496.7022476196289  tr_total:  510.17433404922485  loss:  1404.115478515625\n",
            "Data loader Batch:  82 Time:  6.270216941833496  dl_total:  502.81439089775085  tr_total:  516.4445509910583  loss:  1433.9927978515625\n",
            "Data loader Batch:  83 Time:  6.1586973667144775  dl_total:  508.8189527988434  tr_total:  522.6032483577728  loss:  1572.30322265625\n",
            "Data loader Batch:  84 Time:  6.290727853775024  dl_total:  514.954941034317  tr_total:  528.8939762115479  loss:  1386.7471923828125\n",
            "Data loader Batch:  85 Time:  5.994237184524536  dl_total:  520.8003809452057  tr_total:  534.8882133960724  loss:  1374.5341796875\n",
            "Data loader Batch:  86 Time:  6.31404972076416  dl_total:  526.9519157409668  tr_total:  541.2022631168365  loss:  1413.03125\n",
            "Data loader Batch:  87 Time:  6.122311353683472  dl_total:  532.9233019351959  tr_total:  547.32457447052  loss:  1438.10693359375\n",
            "Data loader Batch:  88 Time:  6.372416257858276  dl_total:  539.1402795314789  tr_total:  553.6969907283783  loss:  1428.5521240234375\n",
            "Data loader Batch:  89 Time:  6.229338645935059  dl_total:  545.2194306850433  tr_total:  559.9263293743134  loss:  1373.66455078125\n",
            "Data loader Batch:  90 Time:  6.3347978591918945  dl_total:  551.3742151260376  tr_total:  566.2611272335052  loss:  1384.4521484375\n",
            "Data loader Batch:  91 Time:  4.8022730350494385  dl_total:  556.1025347709656  tr_total:  571.0634002685547  loss:  1409.5665283203125\n",
            "Epoch: 2; Training Loss: 127069.40405273438 | Validation AP: 0.869 AUC: 0.800\n",
            " Epoch: 2; Test AP: 0.849 AUC: 0.773\n",
            " Epoch: 2; Test New Node AP: 0.864 AUC: 0.795\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:818: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loader Batch:  0 Time:  6.301778554916382  dl_total:  6.1577980518341064  tr_total:  6.301778554916382  loss:  1450.47412109375\n",
            "Data loader Batch:  1 Time:  6.182621002197266  dl_total:  12.192085027694702  tr_total:  12.484399557113647  loss:  1451.831298828125\n",
            "Data loader Batch:  2 Time:  6.225046873092651  dl_total:  18.268280267715454  tr_total:  18.7094464302063  loss:  1354.2354736328125\n",
            "Data loader Batch:  3 Time:  6.182330846786499  dl_total:  24.287883281707764  tr_total:  24.891777276992798  loss:  1364.15869140625\n",
            "Data loader Batch:  4 Time:  6.012247562408447  dl_total:  30.14881205558777  tr_total:  30.904024839401245  loss:  1436.343017578125\n",
            "Data loader Batch:  5 Time:  6.219927787780762  dl_total:  36.207263469696045  tr_total:  37.12395262718201  loss:  1426.390869140625\n",
            "Data loader Batch:  6 Time:  6.252297639846802  dl_total:  42.294633626937866  tr_total:  43.37625026702881  loss:  1431.716796875\n",
            "Data loader Batch:  7 Time:  6.247730255126953  dl_total:  48.380974531173706  tr_total:  49.62398052215576  loss:  1383.8551025390625\n",
            "Data loader Batch:  8 Time:  6.25304913520813  dl_total:  54.474204540252686  tr_total:  55.87702965736389  loss:  1565.0882568359375\n",
            "Data loader Batch:  9 Time:  6.283064603805542  dl_total:  60.59681534767151  tr_total:  62.160094261169434  loss:  1392.430908203125\n",
            "Data loader Batch:  10 Time:  6.449605226516724  dl_total:  66.7216444015503  tr_total:  68.60969948768616  loss:  1366.811279296875\n",
            "Data loader Batch:  11 Time:  6.036270618438721  dl_total:  72.60546684265137  tr_total:  74.64597010612488  loss:  1403.93212890625\n",
            "Data loader Batch:  12 Time:  6.158849239349365  dl_total:  78.60875105857849  tr_total:  80.80481934547424  loss:  1417.3778076171875\n",
            "Data loader Batch:  13 Time:  6.217575311660767  dl_total:  84.6739182472229  tr_total:  87.02239465713501  loss:  1417.94873046875\n",
            "Data loader Batch:  14 Time:  6.129465818405151  dl_total:  90.64901924133301  tr_total:  93.15186047554016  loss:  1417.844482421875\n",
            "Data loader Batch:  15 Time:  6.15269923210144  dl_total:  96.65339779853821  tr_total:  99.3045597076416  loss:  1494.18212890625\n",
            "Data loader Batch:  16 Time:  6.144088983535767  dl_total:  102.6452214717865  tr_total:  105.44864869117737  loss:  1416.8818359375\n",
            "Data loader Batch:  17 Time:  6.2334606647491455  dl_total:  108.72500729560852  tr_total:  111.68210935592651  loss:  1461.80126953125\n",
            "Data loader Batch:  18 Time:  6.1503612995147705  dl_total:  114.73091077804565  tr_total:  117.83247065544128  loss:  1437.23193359375\n",
            "Data loader Batch:  19 Time:  6.225637674331665  dl_total:  120.80875706672668  tr_total:  124.05810832977295  loss:  1389.9871826171875\n",
            "Data loader Batch:  20 Time:  6.159397840499878  dl_total:  126.81567287445068  tr_total:  130.21750617027283  loss:  1374.7984619140625\n",
            "Data loader Batch:  21 Time:  6.287534713745117  dl_total:  132.95333623886108  tr_total:  136.50504088401794  loss:  1404.776123046875\n",
            "Data loader Batch:  22 Time:  6.219440460205078  dl_total:  139.0215437412262  tr_total:  142.72448134422302  loss:  1415.3052978515625\n",
            "Data loader Batch:  23 Time:  6.033321857452393  dl_total:  144.89070439338684  tr_total:  148.75780320167542  loss:  1366.4522705078125\n",
            "Data loader Batch:  24 Time:  6.179114103317261  dl_total:  150.91505026817322  tr_total:  154.93691730499268  loss:  1447.8717041015625\n",
            "Data loader Batch:  25 Time:  6.226704835891724  dl_total:  156.98849725723267  tr_total:  161.1636221408844  loss:  1422.2911376953125\n",
            "Data loader Batch:  26 Time:  6.204266548156738  dl_total:  163.03169512748718  tr_total:  167.36788868904114  loss:  1374.454345703125\n",
            "Data loader Batch:  27 Time:  6.236454963684082  dl_total:  169.10925793647766  tr_total:  173.60434365272522  loss:  1364.657470703125\n",
            "Data loader Batch:  28 Time:  6.139587879180908  dl_total:  175.09544920921326  tr_total:  179.74393153190613  loss:  1360.752197265625\n",
            "Data loader Batch:  29 Time:  6.229723691940308  dl_total:  181.16586184501648  tr_total:  185.97365522384644  loss:  1376.010986328125\n",
            "Data loader Batch:  30 Time:  6.121556043624878  dl_total:  187.12825179100037  tr_total:  192.0952112674713  loss:  1367.2176513671875\n",
            "Data loader Batch:  31 Time:  6.206597566604614  dl_total:  193.17298364639282  tr_total:  198.30180883407593  loss:  1379.79833984375\n",
            "Data loader Batch:  32 Time:  6.128874063491821  dl_total:  199.14743065834045  tr_total:  204.43068289756775  loss:  1348.5286865234375\n",
            "Data loader Batch:  33 Time:  6.088352918624878  dl_total:  205.08970284461975  tr_total:  210.51903581619263  loss:  1371.517333984375\n",
            "Data loader Batch:  34 Time:  6.253238201141357  dl_total:  211.12287998199463  tr_total:  216.77227401733398  loss:  1380.895751953125\n",
            "Data loader Batch:  35 Time:  6.20777702331543  dl_total:  217.18601775169373  tr_total:  222.9800510406494  loss:  1357.195556640625\n",
            "Data loader Batch:  36 Time:  6.2538323402404785  dl_total:  223.22458815574646  tr_total:  229.2338833808899  loss:  1448.4154052734375\n",
            "Data loader Batch:  37 Time:  6.27804160118103  dl_total:  229.32320523262024  tr_total:  235.51192498207092  loss:  1345.662109375\n",
            "Data loader Batch:  38 Time:  6.152779817581177  dl_total:  235.3224196434021  tr_total:  241.6647047996521  loss:  1355.9302978515625\n",
            "Data loader Batch:  39 Time:  6.232078790664673  dl_total:  241.40747237205505  tr_total:  247.89678359031677  loss:  1312.7962646484375\n",
            "Data loader Batch:  40 Time:  6.211918830871582  dl_total:  247.44198083877563  tr_total:  254.10870242118835  loss:  1289.89990234375\n",
            "Data loader Batch:  41 Time:  6.34098744392395  dl_total:  253.63059544563293  tr_total:  260.4496898651123  loss:  1317.484619140625\n",
            "Data loader Batch:  42 Time:  6.213256597518921  dl_total:  259.69770884513855  tr_total:  266.6629464626312  loss:  1315.076171875\n",
            "Data loader Batch:  43 Time:  6.283929109573364  dl_total:  265.83102345466614  tr_total:  272.9468755722046  loss:  1289.6806640625\n",
            "Data loader Batch:  44 Time:  6.015048980712891  dl_total:  271.6883647441864  tr_total:  278.9619245529175  loss:  1321.234375\n",
            "Data loader Batch:  45 Time:  6.217637300491333  dl_total:  277.74360632896423  tr_total:  285.1795618534088  loss:  1300.607177734375\n",
            "Data loader Batch:  46 Time:  6.20360803604126  dl_total:  283.7959141731262  tr_total:  291.3831698894501  loss:  1306.935791015625\n",
            "Data loader Batch:  47 Time:  6.216047286987305  dl_total:  289.85769152641296  tr_total:  297.5992171764374  loss:  1264.3267822265625\n",
            "Data loader Batch:  48 Time:  6.189640998840332  dl_total:  295.88925647735596  tr_total:  303.7888581752777  loss:  1303.99658203125\n",
            "Data loader Batch:  49 Time:  6.2353832721710205  dl_total:  301.9729731082916  tr_total:  310.02424144744873  loss:  1295.216552734375\n",
            "Data loader Batch:  50 Time:  6.176967144012451  dl_total:  307.989470243454  tr_total:  316.2012085914612  loss:  1273.0748291015625\n",
            "Data loader Batch:  51 Time:  6.014034271240234  dl_total:  313.8517801761627  tr_total:  322.2152428627014  loss:  1275.75537109375\n",
            "Data loader Batch:  52 Time:  6.220977306365967  dl_total:  319.91925168037415  tr_total:  328.4362201690674  loss:  1265.149658203125\n",
            "Data loader Batch:  53 Time:  6.208409547805786  dl_total:  325.9715139865875  tr_total:  334.64462971687317  loss:  1302.172607421875\n",
            "Data loader Batch:  54 Time:  6.22146201133728  dl_total:  332.03739285469055  tr_total:  340.86609172821045  loss:  1497.61279296875\n",
            "Data loader Batch:  55 Time:  6.1401002407073975  dl_total:  338.02228689193726  tr_total:  347.00619196891785  loss:  1264.4322509765625\n",
            "Data loader Batch:  56 Time:  6.240861177444458  dl_total:  344.1078803539276  tr_total:  353.2470531463623  loss:  1381.745849609375\n",
            "Data loader Batch:  57 Time:  6.224090814590454  dl_total:  350.17577624320984  tr_total:  359.47114396095276  loss:  1262.7113037109375\n",
            "Data loader Batch:  58 Time:  6.235882520675659  dl_total:  356.26300954818726  tr_total:  365.7070264816284  loss:  1349.3428955078125\n",
            "Data loader Batch:  59 Time:  5.989741563796997  dl_total:  362.0946123600006  tr_total:  371.6967680454254  loss:  1276.40478515625\n",
            "Data loader Batch:  60 Time:  6.203615188598633  dl_total:  368.15275049209595  tr_total:  377.90038323402405  loss:  1259.3814697265625\n",
            "Data loader Batch:  61 Time:  6.142487525939941  dl_total:  374.14534401893616  tr_total:  384.042870759964  loss:  1308.88623046875\n",
            "Data loader Batch:  62 Time:  6.249036550521851  dl_total:  380.2386095523834  tr_total:  390.29190731048584  loss:  1328.2803955078125\n",
            "Data loader Batch:  63 Time:  6.160157918930054  dl_total:  386.24648213386536  tr_total:  396.4520652294159  loss:  1326.21484375\n",
            "Data loader Batch:  64 Time:  6.326509237289429  dl_total:  392.41589999198914  tr_total:  402.7785744667053  loss:  1280.853515625\n",
            "Data loader Batch:  65 Time:  6.338733196258545  dl_total:  398.5997760295868  tr_total:  409.11730766296387  loss:  1272.454345703125\n",
            "Data loader Batch:  66 Time:  6.315524339675903  dl_total:  404.7647075653076  tr_total:  415.43283200263977  loss:  1293.71630859375\n",
            "Data loader Batch:  67 Time:  6.152677059173584  dl_total:  410.7534656524658  tr_total:  421.58550906181335  loss:  1272.921630859375\n",
            "Data loader Batch:  68 Time:  6.352235555648804  dl_total:  416.9531092643738  tr_total:  427.93774461746216  loss:  1242.91943359375\n",
            "Data loader Batch:  69 Time:  6.209462404251099  dl_total:  422.84341192245483  tr_total:  434.14720702171326  loss:  1331.9295654296875\n",
            "Data loader Batch:  70 Time:  6.072237491607666  dl_total:  428.76392126083374  tr_total:  440.2194445133209  loss:  1267.7108154296875\n",
            "Data loader Batch:  71 Time:  6.203619718551636  dl_total:  434.8145055770874  tr_total:  446.42306423187256  loss:  1320.9132080078125\n",
            "Data loader Batch:  72 Time:  6.2719566822052  dl_total:  440.9314684867859  tr_total:  452.69502091407776  loss:  1236.337646484375\n",
            "Data loader Batch:  73 Time:  6.146815538406372  dl_total:  446.9286549091339  tr_total:  458.84183645248413  loss:  1267.066162109375\n",
            "Data loader Batch:  74 Time:  6.290564060211182  dl_total:  453.0697343349457  tr_total:  465.1324005126953  loss:  1330.9246826171875\n",
            "Data loader Batch:  75 Time:  6.266076564788818  dl_total:  459.1766197681427  tr_total:  471.39847707748413  loss:  1242.2486572265625\n",
            "Data loader Batch:  76 Time:  6.353411674499512  dl_total:  465.37420535087585  tr_total:  477.75188875198364  loss:  1319.2607421875\n",
            "Data loader Batch:  77 Time:  6.203726053237915  dl_total:  471.41864490509033  tr_total:  483.95561480522156  loss:  1345.9935302734375\n",
            "Data loader Batch:  78 Time:  6.227409601211548  dl_total:  477.4958755970001  tr_total:  490.1830244064331  loss:  1312.843017578125\n",
            "Data loader Batch:  79 Time:  6.286352872848511  dl_total:  483.6130540370941  tr_total:  496.4693772792816  loss:  1386.91259765625\n",
            "Data loader Batch:  80 Time:  6.343709468841553  dl_total:  489.79879808425903  tr_total:  502.81308674812317  loss:  1310.3466796875\n",
            "Data loader Batch:  81 Time:  6.249699831008911  dl_total:  495.8780496120453  tr_total:  509.0627865791321  loss:  1320.808837890625\n",
            "Data loader Batch:  82 Time:  6.272779226303101  dl_total:  501.9785897731781  tr_total:  515.3355658054352  loss:  1379.7607421875\n",
            "Data loader Batch:  83 Time:  6.239776134490967  dl_total:  508.05850863456726  tr_total:  521.5753419399261  loss:  1462.7498779296875\n",
            "Data loader Batch:  84 Time:  6.25705623626709  dl_total:  514.1535377502441  tr_total:  527.8323981761932  loss:  1386.946533203125\n",
            "Data loader Batch:  85 Time:  6.36178731918335  dl_total:  520.1287257671356  tr_total:  534.1941854953766  loss:  1280.163330078125\n",
            "Data loader Batch:  86 Time:  6.23376989364624  dl_total:  526.2108759880066  tr_total:  540.4279553890228  loss:  1380.5966796875\n",
            "Data loader Batch:  87 Time:  6.075084686279297  dl_total:  532.136488199234  tr_total:  546.5030400753021  loss:  1320.5003662109375\n",
            "Data loader Batch:  88 Time:  6.187410116195679  dl_total:  538.1786417961121  tr_total:  552.6904501914978  loss:  1289.581787109375\n",
            "Data loader Batch:  89 Time:  6.237897634506226  dl_total:  544.2636365890503  tr_total:  558.928347826004  loss:  1486.444580078125\n",
            "Data loader Batch:  90 Time:  6.1900718212127686  dl_total:  550.3023529052734  tr_total:  565.1184196472168  loss:  1386.95654296875\n",
            "Data loader Batch:  91 Time:  5.06902003288269  dl_total:  555.2988543510437  tr_total:  570.1874396800995  loss:  1339.5335693359375\n",
            "Epoch: 3; Training Loss: 124500.86999511719 | Validation AP: 0.863 AUC: 0.791\n",
            " Epoch: 3; Test AP: 0.819 AUC: 0.736\n",
            " Epoch: 3; Test New Node AP: 0.838 AUC: 0.754\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:818: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loader Batch:  0 Time:  6.0008721351623535  dl_total:  5.856108903884888  tr_total:  6.0008721351623535  loss:  1562.5166015625\n",
            "Data loader Batch:  1 Time:  6.208112478256226  dl_total:  11.903465986251831  tr_total:  12.208984613418579  loss:  1244.8040771484375\n",
            "Data loader Batch:  2 Time:  6.148840427398682  dl_total:  17.889243602752686  tr_total:  18.35782504081726  loss:  1235.9228515625\n",
            "Data loader Batch:  3 Time:  6.197173357009888  dl_total:  23.93748140335083  tr_total:  24.55499839782715  loss:  1368.166748046875\n",
            "Data loader Batch:  4 Time:  6.074987888336182  dl_total:  29.85518765449524  tr_total:  30.62998628616333  loss:  1424.8466796875\n",
            "Data loader Batch:  5 Time:  6.201061964035034  dl_total:  35.90401554107666  tr_total:  36.831048250198364  loss:  1318.6396484375\n",
            "Data loader Batch:  6 Time:  6.198243141174316  dl_total:  41.87521934509277  tr_total:  43.02929139137268  loss:  1376.8466796875\n",
            "Data loader Batch:  7 Time:  6.268340587615967  dl_total:  47.986974000930786  tr_total:  49.29763197898865  loss:  1355.745849609375\n",
            "Data loader Batch:  8 Time:  5.982712268829346  dl_total:  53.815956592559814  tr_total:  55.28034424781799  loss:  1603.427978515625\n",
            "Data loader Batch:  9 Time:  6.347492218017578  dl_total:  60.0119743347168  tr_total:  61.62783646583557  loss:  1375.96728515625\n",
            "Data loader Batch:  10 Time:  6.204039812088013  dl_total:  66.06458306312561  tr_total:  67.83187627792358  loss:  1361.57275390625\n",
            "Data loader Batch:  11 Time:  6.294390678405762  dl_total:  72.21360754966736  tr_total:  74.12626695632935  loss:  1407.985107421875\n",
            "Data loader Batch:  12 Time:  6.17326545715332  dl_total:  78.22622036933899  tr_total:  80.29953241348267  loss:  1524.4864501953125\n",
            "Data loader Batch:  13 Time:  6.222813129425049  dl_total:  84.29764246940613  tr_total:  86.52234554290771  loss:  1347.81787109375\n",
            "Data loader Batch:  14 Time:  6.1641764640808105  dl_total:  90.30700874328613  tr_total:  92.68652200698853  loss:  1306.398193359375\n",
            "Data loader Batch:  15 Time:  6.193023681640625  dl_total:  96.35178446769714  tr_total:  98.87954568862915  loss:  1492.8626708984375\n",
            "Data loader Batch:  16 Time:  6.158869981765747  dl_total:  102.36131691932678  tr_total:  105.0384156703949  loss:  1421.1883544921875\n",
            "Data loader Batch:  17 Time:  6.2799811363220215  dl_total:  108.48965764045715  tr_total:  111.31839680671692  loss:  1386.60498046875\n",
            "Data loader Batch:  18 Time:  5.921201944351196  dl_total:  114.26330518722534  tr_total:  117.23959875106812  loss:  1442.223876953125\n",
            "Data loader Batch:  19 Time:  6.2141406536102295  dl_total:  120.3285140991211  tr_total:  123.45373940467834  loss:  1359.223388671875\n",
            "Data loader Batch:  20 Time:  6.139462232589722  dl_total:  126.31826305389404  tr_total:  129.59320163726807  loss:  1341.335205078125\n",
            "Data loader Batch:  21 Time:  6.236275672912598  dl_total:  132.40376257896423  tr_total:  135.82947731018066  loss:  1418.6907958984375\n",
            "Data loader Batch:  22 Time:  6.1880576610565186  dl_total:  138.43822288513184  tr_total:  142.01753497123718  loss:  1387.9495849609375\n",
            "Data loader Batch:  23 Time:  6.245861768722534  dl_total:  144.5280261039734  tr_total:  148.26339673995972  loss:  1334.934814453125\n",
            "Data loader Batch:  24 Time:  6.162045001983643  dl_total:  150.5024609565735  tr_total:  154.42544174194336  loss:  1473.045654296875\n",
            "Data loader Batch:  25 Time:  6.200037479400635  dl_total:  156.54819440841675  tr_total:  160.625479221344  loss:  1360.15966796875\n",
            "Data loader Batch:  26 Time:  6.163059949874878  dl_total:  162.56380891799927  tr_total:  166.78853917121887  loss:  1265.144287109375\n",
            "Data loader Batch:  27 Time:  6.242464065551758  dl_total:  168.6469669342041  tr_total:  173.03100323677063  loss:  1366.6480712890625\n",
            "Data loader Batch:  28 Time:  6.149628400802612  dl_total:  174.6388189792633  tr_total:  179.18063163757324  loss:  1375.0\n",
            "Data loader Batch:  29 Time:  6.002570390701294  dl_total:  180.49336290359497  tr_total:  185.18320202827454  loss:  1350.052978515625\n",
            "Data loader Batch:  30 Time:  6.1774656772613525  dl_total:  186.50206422805786  tr_total:  191.3606677055359  loss:  1418.7220458984375\n",
            "Data loader Batch:  31 Time:  6.176630258560181  dl_total:  192.50380420684814  tr_total:  197.53729796409607  loss:  1375.4757080078125\n",
            "Data loader Batch:  32 Time:  6.206437587738037  dl_total:  198.54581904411316  tr_total:  203.7437355518341  loss:  1303.393798828125\n",
            "Data loader Batch:  33 Time:  6.158718109130859  dl_total:  204.56149792671204  tr_total:  209.90245366096497  loss:  1370.6600341796875\n",
            "Data loader Batch:  34 Time:  6.192803859710693  dl_total:  210.58909511566162  tr_total:  216.09525752067566  loss:  1284.227783203125\n",
            "Data loader Batch:  35 Time:  6.243286371231079  dl_total:  216.66655564308167  tr_total:  222.33854389190674  loss:  1369.7564697265625\n",
            "Data loader Batch:  36 Time:  6.218459844589233  dl_total:  222.70479559898376  tr_total:  228.55700373649597  loss:  1351.728759765625\n",
            "Data loader Batch:  37 Time:  6.2556984424591064  dl_total:  228.80491590499878  tr_total:  234.81270217895508  loss:  1307.2132568359375\n",
            "Data loader Batch:  38 Time:  6.29339337348938  dl_total:  234.91686606407166  tr_total:  241.10609555244446  loss:  1318.3690185546875\n",
            "Data loader Batch:  39 Time:  6.140456438064575  dl_total:  240.89049124717712  tr_total:  247.24655199050903  loss:  1311.364990234375\n",
            "Data loader Batch:  40 Time:  6.265262603759766  dl_total:  247.00705862045288  tr_total:  253.5118145942688  loss:  1295.51318359375\n",
            "Data loader Batch:  41 Time:  6.215769529342651  dl_total:  253.07709646224976  tr_total:  259.72758412361145  loss:  1263.0035400390625\n",
            "Data loader Batch:  42 Time:  6.0096211433410645  dl_total:  258.9393243789673  tr_total:  265.7372052669525  loss:  1274.516357421875\n",
            "Data loader Batch:  43 Time:  6.1807990074157715  dl_total:  264.9665951728821  tr_total:  271.9180042743683  loss:  1288.417236328125\n",
            "Data loader Batch:  44 Time:  6.285474538803101  dl_total:  271.09359860420227  tr_total:  278.2034788131714  loss:  1299.05908203125\n",
            "Data loader Batch:  45 Time:  6.128567695617676  dl_total:  277.0649528503418  tr_total:  284.33204650878906  loss:  1308.52099609375\n",
            "Data loader Batch:  46 Time:  6.229523181915283  dl_total:  283.13971400260925  tr_total:  290.56156969070435  loss:  1213.3251953125\n",
            "Data loader Batch:  47 Time:  6.12797999382019  dl_total:  289.11115074157715  tr_total:  296.68954968452454  loss:  1265.692138671875\n",
            "Data loader Batch:  48 Time:  6.229703187942505  dl_total:  295.1753821372986  tr_total:  302.91925287246704  loss:  1249.3668212890625\n",
            "Data loader Batch:  49 Time:  6.2632737159729  dl_total:  301.2848048210144  tr_total:  309.18252658843994  loss:  1247.3031005859375\n",
            "Data loader Batch:  50 Time:  6.024800062179565  dl_total:  307.1511218547821  tr_total:  315.2073266506195  loss:  1254.26611328125\n",
            "Data loader Batch:  51 Time:  6.151659727096558  dl_total:  313.1494119167328  tr_total:  321.35898637771606  loss:  1217.4765625\n",
            "Data loader Batch:  52 Time:  6.322193384170532  dl_total:  319.285596370697  tr_total:  327.6811797618866  loss:  1267.91357421875\n",
            "Data loader Batch:  53 Time:  6.176982402801514  dl_total:  325.30942368507385  tr_total:  333.8581621646881  loss:  1286.743896484375\n",
            "Data loader Batch:  54 Time:  6.278894424438477  dl_total:  331.4416136741638  tr_total:  340.1370565891266  loss:  1462.7841796875\n",
            "Data loader Batch:  55 Time:  6.235596179962158  dl_total:  337.5212960243225  tr_total:  346.37265276908875  loss:  1252.479248046875\n",
            "Data loader Batch:  56 Time:  6.236374378204346  dl_total:  343.6011347770691  tr_total:  352.6090271472931  loss:  1233.955322265625\n",
            "Data loader Batch:  57 Time:  6.273192405700684  dl_total:  349.7063331604004  tr_total:  358.8822195529938  loss:  1186.2469482421875\n",
            "Data loader Batch:  58 Time:  6.3176140785217285  dl_total:  355.8706171512604  tr_total:  365.1998336315155  loss:  1244.669921875\n",
            "Data loader Batch:  59 Time:  6.264549016952515  dl_total:  361.9644124507904  tr_total:  371.464382648468  loss:  1170.64404296875\n",
            "Data loader Batch:  60 Time:  6.08942723274231  dl_total:  367.89975333213806  tr_total:  377.5538098812103  loss:  1240.2510986328125\n",
            "Data loader Batch:  61 Time:  6.22899055480957  dl_total:  373.9823079109192  tr_total:  383.7828004360199  loss:  1274.29541015625\n",
            "Data loader Batch:  62 Time:  6.25995135307312  dl_total:  380.0926914215088  tr_total:  390.042751789093  loss:  1260.123779296875\n",
            "Data loader Batch:  63 Time:  6.239476680755615  dl_total:  386.17453360557556  tr_total:  396.28222846984863  loss:  1262.4388427734375\n",
            "Data loader Batch:  64 Time:  6.288336277008057  dl_total:  392.3136074542999  tr_total:  402.5705647468567  loss:  1248.941650390625\n",
            "Data loader Batch:  65 Time:  6.257818698883057  dl_total:  398.41585063934326  tr_total:  408.82838344573975  loss:  1169.6593017578125\n",
            "Data loader Batch:  66 Time:  6.332104206085205  dl_total:  404.5895907878876  tr_total:  415.16048765182495  loss:  1218.0535888671875\n",
            "Data loader Batch:  67 Time:  6.24043345451355  dl_total:  410.66347670555115  tr_total:  421.4009211063385  loss:  1235.921142578125\n",
            "Data loader Batch:  68 Time:  6.254754066467285  dl_total:  416.75500774383545  tr_total:  427.6556751728058  loss:  1243.7598876953125\n",
            "Data loader Batch:  69 Time:  6.203266859054565  dl_total:  422.79683923721313  tr_total:  433.85894203186035  loss:  1227.214111328125\n",
            "Data loader Batch:  70 Time:  6.013885974884033  dl_total:  428.6578280925751  tr_total:  439.8728280067444  loss:  1270.909912109375\n",
            "Data loader Batch:  71 Time:  6.224863052368164  dl_total:  434.7205936908722  tr_total:  446.09769105911255  loss:  1221.194580078125\n",
            "Data loader Batch:  72 Time:  6.191571950912476  dl_total:  440.6992690563202  tr_total:  452.289263010025  loss:  1261.041748046875\n",
            "Data loader Batch:  73 Time:  6.187859058380127  dl_total:  446.7096095085144  tr_total:  458.47712206840515  loss:  1235.2467041015625\n",
            "Data loader Batch:  74 Time:  6.19168496131897  dl_total:  452.75194025039673  tr_total:  464.6688070297241  loss:  1277.4053955078125\n",
            "Data loader Batch:  75 Time:  6.313282012939453  dl_total:  458.91157698631287  tr_total:  470.9820890426636  loss:  1227.5501708984375\n",
            "Data loader Batch:  76 Time:  6.283657550811768  dl_total:  465.03029704093933  tr_total:  477.26574659347534  loss:  1273.67333984375\n",
            "Data loader Batch:  77 Time:  6.27930760383606  dl_total:  471.1637818813324  tr_total:  483.5450541973114  loss:  1253.668212890625\n",
            "Data loader Batch:  78 Time:  6.2218921184539795  dl_total:  477.2354106903076  tr_total:  489.7669463157654  loss:  1342.4219970703125\n",
            "Data loader Batch:  79 Time:  6.151247024536133  dl_total:  483.2432155609131  tr_total:  495.9181933403015  loss:  1302.12158203125\n",
            "Data loader Batch:  80 Time:  6.332752227783203  dl_total:  489.4244079589844  tr_total:  502.2509455680847  loss:  1259.265625\n",
            "Data loader Batch:  81 Time:  6.2331671714782715  dl_total:  495.50521326065063  tr_total:  508.484112739563  loss:  1360.303955078125\n",
            "Data loader Batch:  82 Time:  6.216400623321533  dl_total:  501.52302956581116  tr_total:  514.7005133628845  loss:  1223.67578125\n",
            "Data loader Batch:  83 Time:  6.220335245132446  dl_total:  507.5820071697235  tr_total:  520.920848608017  loss:  1453.2689208984375\n",
            "Data loader Batch:  84 Time:  6.211756706237793  dl_total:  513.6295123100281  tr_total:  527.1326053142548  loss:  1347.2342529296875\n",
            "Data loader Batch:  85 Time:  6.281066656112671  dl_total:  519.7597486972809  tr_total:  533.4136719703674  loss:  1307.5814208984375\n",
            "Data loader Batch:  86 Time:  6.267635822296143  dl_total:  525.8657386302948  tr_total:  539.6813077926636  loss:  1292.02685546875\n",
            "Data loader Batch:  87 Time:  6.29715633392334  dl_total:  532.0049438476562  tr_total:  545.9784641265869  loss:  1405.8717041015625\n",
            "Data loader Batch:  88 Time:  6.071861982345581  dl_total:  537.916752576828  tr_total:  552.0503261089325  loss:  1302.402587890625\n",
            "Data loader Batch:  89 Time:  6.328331470489502  dl_total:  544.0904350280762  tr_total:  558.378657579422  loss:  1382.2818603515625\n",
            "Data loader Batch:  90 Time:  6.220818519592285  dl_total:  550.1631624698639  tr_total:  564.5994760990143  loss:  1341.69921875\n",
            "Data loader Batch:  91 Time:  5.068263053894043  dl_total:  555.1612343788147  tr_total:  569.6677391529083  loss:  1331.2508544921875\n",
            "Epoch: 4; Training Loss: 121335.77392578125 | Validation AP: 0.873 AUC: 0.814\n",
            " Epoch: 4; Test AP: 0.822 AUC: 0.744\n",
            " Epoch: 4; Test New Node AP: 0.838 AUC: 0.760\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:818: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loader Batch:  0 Time:  6.308718919754028  dl_total:  6.139158248901367  tr_total:  6.308718919754028  loss:  1450.4423828125\n",
            "Data loader Batch:  1 Time:  6.259567499160767  dl_total:  12.242077589035034  tr_total:  12.568286418914795  loss:  1340.667724609375\n",
            "Data loader Batch:  2 Time:  5.983293533325195  dl_total:  18.07577085494995  tr_total:  18.55157995223999  loss:  1263.19775390625\n",
            "Data loader Batch:  3 Time:  6.212126731872559  dl_total:  24.13049602508545  tr_total:  24.76370668411255  loss:  1290.6575927734375\n",
            "Data loader Batch:  4 Time:  6.152943849563599  dl_total:  30.126096725463867  tr_total:  30.916650533676147  loss:  1298.635009765625\n",
            "Data loader Batch:  5 Time:  6.135547876358032  dl_total:  36.11189675331116  tr_total:  37.05219841003418  loss:  1277.7816162109375\n",
            "Data loader Batch:  6 Time:  6.127482652664185  dl_total:  42.0844612121582  tr_total:  43.179681062698364  loss:  1281.990966796875\n",
            "Data loader Batch:  7 Time:  6.2112531661987305  dl_total:  48.13984560966492  tr_total:  49.390934228897095  loss:  1386.4989013671875\n",
            "Data loader Batch:  8 Time:  6.151325225830078  dl_total:  54.129019260406494  tr_total:  55.54225945472717  loss:  1314.124267578125\n",
            "Data loader Batch:  9 Time:  6.238537073135376  dl_total:  60.20559620857239  tr_total:  61.78079652786255  loss:  1322.2462158203125\n",
            "Data loader Batch:  10 Time:  6.2429282665252686  dl_total:  66.28949308395386  tr_total:  68.02372479438782  loss:  1303.671630859375\n",
            "Data loader Batch:  11 Time:  6.229068756103516  dl_total:  72.36583018302917  tr_total:  74.25279355049133  loss:  1362.705322265625\n",
            "Data loader Batch:  12 Time:  6.02492880821228  dl_total:  78.22463417053223  tr_total:  80.27772235870361  loss:  1358.1510009765625\n",
            "Data loader Batch:  13 Time:  6.1343066692352295  dl_total:  84.19625043869019  tr_total:  86.41202902793884  loss:  1339.3787841796875\n",
            "Data loader Batch:  14 Time:  6.1593663692474365  dl_total:  90.18362212181091  tr_total:  92.57139539718628  loss:  1253.104248046875\n",
            "Data loader Batch:  15 Time:  6.1575927734375  dl_total:  96.18975782394409  tr_total:  98.72898817062378  loss:  1423.3292236328125\n",
            "Data loader Batch:  16 Time:  6.271062850952148  dl_total:  102.30903148651123  tr_total:  105.00005102157593  loss:  1366.01171875\n",
            "Data loader Batch:  17 Time:  6.180737018585205  dl_total:  108.33922696113586  tr_total:  111.18078804016113  loss:  1399.21337890625\n",
            "Data loader Batch:  18 Time:  6.040825843811035  dl_total:  114.23348689079285  tr_total:  117.22161388397217  loss:  1320.825439453125\n",
            "Data loader Batch:  19 Time:  6.19542670249939  dl_total:  120.27762460708618  tr_total:  123.41704058647156  loss:  1303.0625\n",
            "Data loader Batch:  20 Time:  6.251973867416382  dl_total:  126.37183856964111  tr_total:  129.66901445388794  loss:  1273.1053466796875\n",
            "Data loader Batch:  21 Time:  6.174557209014893  dl_total:  132.37750840187073  tr_total:  135.84357166290283  loss:  1295.902587890625\n",
            "Data loader Batch:  22 Time:  6.252439737319946  dl_total:  138.4678590297699  tr_total:  142.09601140022278  loss:  1318.7919921875\n",
            "Data loader Batch:  23 Time:  6.184743642807007  dl_total:  144.49145030975342  tr_total:  148.28075504302979  loss:  1286.7662353515625\n",
            "Data loader Batch:  24 Time:  6.2050580978393555  dl_total:  150.53917407989502  tr_total:  154.48581314086914  loss:  1317.3231201171875\n",
            "Data loader Batch:  25 Time:  6.122737169265747  dl_total:  156.5074121952057  tr_total:  160.6085503101349  loss:  1330.23291015625\n",
            "Data loader Batch:  26 Time:  6.163942337036133  dl_total:  162.5216817855835  tr_total:  166.77249264717102  loss:  1311.446044921875\n",
            "Data loader Batch:  27 Time:  6.190380096435547  dl_total:  168.55206322669983  tr_total:  172.96287274360657  loss:  1335.168701171875\n",
            "Data loader Batch:  28 Time:  6.2244861125946045  dl_total:  174.61849355697632  tr_total:  179.18735885620117  loss:  1299.4840087890625\n",
            "Data loader Batch:  29 Time:  5.960858345031738  dl_total:  180.4304404258728  tr_total:  185.1482172012329  loss:  1243.308837890625\n",
            "Data loader Batch:  30 Time:  6.211887836456299  dl_total:  186.4884295463562  tr_total:  191.3601050376892  loss:  1261.5374755859375\n",
            "Data loader Batch:  31 Time:  6.150855302810669  dl_total:  192.4837954044342  tr_total:  197.51096034049988  loss:  1296.1474609375\n",
            "Data loader Batch:  32 Time:  6.200079679489136  dl_total:  198.5328483581543  tr_total:  203.711040019989  loss:  1342.4903564453125\n",
            "Data loader Batch:  33 Time:  6.387109279632568  dl_total:  204.59568977355957  tr_total:  210.09814929962158  loss:  1283.95166015625\n",
            "Data loader Batch:  34 Time:  6.038242816925049  dl_total:  210.47892260551453  tr_total:  216.13639211654663  loss:  1248.0091552734375\n",
            "Data loader Batch:  35 Time:  6.093027591705322  dl_total:  216.42198753356934  tr_total:  222.22941970825195  loss:  1305.321533203125\n",
            "Data loader Batch:  36 Time:  6.295408725738525  dl_total:  222.5664131641388  tr_total:  228.52482843399048  loss:  1220.619873046875\n",
            "Data loader Batch:  37 Time:  6.179703712463379  dl_total:  228.5820972919464  tr_total:  234.70453214645386  loss:  1187.9844970703125\n",
            "Data loader Batch:  38 Time:  6.198745489120483  dl_total:  234.62979435920715  tr_total:  240.90327763557434  loss:  1134.1453857421875\n",
            "Data loader Batch:  39 Time:  6.089193820953369  dl_total:  240.5721402168274  tr_total:  246.9924714565277  loss:  1179.218017578125\n",
            "Data loader Batch:  40 Time:  6.246534109115601  dl_total:  246.66688561439514  tr_total:  253.2390055656433  loss:  1178.6376953125\n",
            "Data loader Batch:  41 Time:  6.2010252475738525  dl_total:  252.71803617477417  tr_total:  259.44003081321716  loss:  1201.765625\n",
            "Data loader Batch:  42 Time:  6.301180362701416  dl_total:  258.8762791156769  tr_total:  265.7412111759186  loss:  1249.1309814453125\n",
            "Data loader Batch:  43 Time:  6.138487100601196  dl_total:  264.86705136299133  tr_total:  271.8796982765198  loss:  1266.904052734375\n",
            "Data loader Batch:  44 Time:  6.2656471729278564  dl_total:  270.976598739624  tr_total:  278.14534544944763  loss:  1201.7757568359375\n",
            "Data loader Batch:  45 Time:  5.970712423324585  dl_total:  276.7874743938446  tr_total:  284.1160578727722  loss:  1201.1822509765625\n",
            "Data loader Batch:  46 Time:  6.200037717819214  dl_total:  282.8301920890808  tr_total:  290.31609559059143  loss:  1136.36865234375\n",
            "Data loader Batch:  47 Time:  6.1119606494903564  dl_total:  288.7815625667572  tr_total:  296.4280562400818  loss:  1216.378173828125\n",
            "Data loader Batch:  48 Time:  6.225483655929565  dl_total:  294.85113310813904  tr_total:  302.65353989601135  loss:  1255.45166015625\n",
            "Data loader Batch:  49 Time:  6.2186455726623535  dl_total:  300.91502714157104  tr_total:  308.8721854686737  loss:  1261.344970703125\n",
            "Data loader Batch:  50 Time:  6.236196994781494  dl_total:  306.9942240715027  tr_total:  315.1083824634552  loss:  1115.9227294921875\n",
            "Data loader Batch:  51 Time:  6.280822992324829  dl_total:  313.1167893409729  tr_total:  321.38920545578003  loss:  1191.50439453125\n",
            "Data loader Batch:  52 Time:  6.280605792999268  dl_total:  319.23758912086487  tr_total:  327.6698112487793  loss:  1154.8935546875\n",
            "Data loader Batch:  53 Time:  6.230687141418457  dl_total:  325.3082640171051  tr_total:  333.90049839019775  loss:  1211.9012451171875\n",
            "Data loader Batch:  54 Time:  5.970869779586792  dl_total:  331.12981271743774  tr_total:  339.87136816978455  loss:  1555.9791259765625\n",
            "Data loader Batch:  55 Time:  6.2291340827941895  dl_total:  337.20139026641846  tr_total:  346.10050225257874  loss:  1222.702880859375\n",
            "Data loader Batch:  56 Time:  6.2764892578125  dl_total:  343.32348370552063  tr_total:  352.37699151039124  loss:  1210.6900634765625\n",
            "Data loader Batch:  57 Time:  6.361330270767212  dl_total:  349.51007175445557  tr_total:  358.73832178115845  loss:  1243.870361328125\n",
            "Data loader Batch:  58 Time:  6.197571516036987  dl_total:  355.5457684993744  tr_total:  364.93589329719543  loss:  1217.9632568359375\n",
            "Data loader Batch:  59 Time:  6.295148611068726  dl_total:  361.6809501647949  tr_total:  371.23104190826416  loss:  1148.1865234375\n",
            "Data loader Batch:  60 Time:  6.143475532531738  dl_total:  367.672482252121  tr_total:  377.3745174407959  loss:  1145.1409912109375\n",
            "Data loader Batch:  61 Time:  6.239455461502075  dl_total:  373.7616813182831  tr_total:  383.613972902298  loss:  1190.26611328125\n",
            "Data loader Batch:  62 Time:  5.959670066833496  dl_total:  379.5697295665741  tr_total:  389.57364296913147  loss:  1271.633056640625\n",
            "Data loader Batch:  63 Time:  6.258377552032471  dl_total:  385.6760022640228  tr_total:  395.83202052116394  loss:  1217.967041015625\n",
            "Data loader Batch:  64 Time:  6.192087173461914  dl_total:  391.71737265586853  tr_total:  402.02410769462585  loss:  1207.02880859375\n",
            "Data loader Batch:  65 Time:  6.516903877258301  dl_total:  397.9020040035248  tr_total:  408.54101157188416  loss:  1132.169921875\n",
            "Data loader Batch:  66 Time:  6.246656894683838  dl_total:  403.81920742988586  tr_total:  414.787668466568  loss:  1187.775146484375\n",
            "Data loader Batch:  67 Time:  6.086774587631226  dl_total:  409.74944376945496  tr_total:  420.8744430541992  loss:  1261.7548828125\n",
            "Data loader Batch:  68 Time:  6.17252779006958  dl_total:  415.7671732902527  tr_total:  427.0469708442688  loss:  1164.428466796875\n",
            "Data loader Batch:  69 Time:  6.366593599319458  dl_total:  421.9735794067383  tr_total:  433.41356444358826  loss:  1271.4881591796875\n",
            "Data loader Batch:  70 Time:  6.179601669311523  dl_total:  428.0027663707733  tr_total:  439.5931661128998  loss:  1211.69921875\n",
            "Data loader Batch:  71 Time:  6.283766031265259  dl_total:  434.1401484012604  tr_total:  445.87693214416504  loss:  1293.4072265625\n",
            "Data loader Batch:  72 Time:  6.148386240005493  dl_total:  440.14061164855957  tr_total:  452.02531838417053  loss:  1242.014404296875\n",
            "Data loader Batch:  73 Time:  6.176265239715576  dl_total:  446.1694951057434  tr_total:  458.2015836238861  loss:  1137.8206787109375\n",
            "Data loader Batch:  74 Time:  6.191305160522461  dl_total:  452.2156205177307  tr_total:  464.39288878440857  loss:  1181.030029296875\n",
            "Data loader Batch:  75 Time:  6.312020301818848  dl_total:  458.3475844860077  tr_total:  470.7049090862274  loss:  1097.8486328125\n",
            "Data loader Batch:  76 Time:  6.274928569793701  dl_total:  464.46798300743103  tr_total:  476.9798376560211  loss:  1268.123046875\n",
            "Data loader Batch:  77 Time:  6.243716716766357  dl_total:  470.5650677680969  tr_total:  483.2235543727875  loss:  1197.944580078125\n",
            "Data loader Batch:  78 Time:  6.005179405212402  dl_total:  476.4202229976654  tr_total:  489.2287337779999  loss:  1306.0498046875\n",
            "Data loader Batch:  79 Time:  6.3492467403411865  dl_total:  482.6205565929413  tr_total:  495.57798051834106  loss:  1200.95166015625\n",
            "Data loader Batch:  80 Time:  6.309689283370972  dl_total:  488.76526737213135  tr_total:  501.88766980171204  loss:  1119.487548828125\n",
            "Data loader Batch:  81 Time:  6.26697301864624  dl_total:  494.87423610687256  tr_total:  508.1546428203583  loss:  1204.7689208984375\n",
            "Data loader Batch:  82 Time:  6.1766135692596436  dl_total:  500.8974084854126  tr_total:  514.3312563896179  loss:  1285.4378662109375\n",
            "Data loader Batch:  83 Time:  6.204894781112671  dl_total:  506.94735741615295  tr_total:  520.5361511707306  loss:  1272.6761474609375\n",
            "Data loader Batch:  84 Time:  6.226978063583374  dl_total:  512.9925503730774  tr_total:  526.763129234314  loss:  1300.9481201171875\n",
            "Data loader Batch:  85 Time:  6.332037925720215  dl_total:  519.1759111881256  tr_total:  533.0951671600342  loss:  1302.8131103515625\n",
            "Data loader Batch:  86 Time:  6.0131237506866455  dl_total:  525.0283269882202  tr_total:  539.1082909107208  loss:  1317.41357421875\n",
            "Data loader Batch:  87 Time:  6.221211194992065  dl_total:  531.1012756824493  tr_total:  545.3295021057129  loss:  1269.3599853515625\n",
            "Data loader Batch:  88 Time:  6.29179310798645  dl_total:  537.241895198822  tr_total:  551.6212952136993  loss:  1256.9725341796875\n",
            "Data loader Batch:  89 Time:  6.228633165359497  dl_total:  543.3173313140869  tr_total:  557.8499283790588  loss:  1372.56298828125\n",
            "Data loader Batch:  90 Time:  6.359317064285278  dl_total:  549.5160155296326  tr_total:  564.2092454433441  loss:  1291.858642578125\n",
            "Data loader Batch:  91 Time:  5.019275903701782  dl_total:  554.4653923511505  tr_total:  569.2285213470459  loss:  1251.197998046875\n",
            "Epoch: 5; Training Loss: 116001.244140625 | Validation AP: 0.850 AUC: 0.783\n",
            " Epoch: 5; Test AP: 0.838 AUC: 0.777\n",
            " Epoch: 5; Test New Node AP: 0.845 AUC: 0.784\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py:818: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py:92: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  if p.grad is not None:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loader Batch:  0 Time:  6.2715044021606445  dl_total:  6.119182109832764  tr_total:  6.2715044021606445  loss:  1403.3604736328125\n",
            "Data loader Batch:  1 Time:  6.158106803894043  dl_total:  12.118695735931396  tr_total:  12.429611206054688  loss:  1244.132080078125\n",
            "Data loader Batch:  2 Time:  6.174557447433472  dl_total:  18.141486167907715  tr_total:  18.60416865348816  loss:  1171.12646484375\n",
            "Data loader Batch:  3 Time:  6.113211631774902  dl_total:  24.08805227279663  tr_total:  24.71738028526306  loss:  1331.2384033203125\n",
            "Data loader Batch:  4 Time:  6.213238000869751  dl_total:  30.151872873306274  tr_total:  30.930618286132812  loss:  1277.6956787109375\n",
            "Data loader Batch:  5 Time:  6.226851940155029  dl_total:  36.21554446220398  tr_total:  37.15747022628784  loss:  1300.624267578125\n",
            "Data loader Batch:  6 Time:  6.243377208709717  dl_total:  42.30465602874756  tr_total:  43.40084743499756  loss:  1261.029296875\n",
            "Data loader Batch:  7 Time:  6.127832412719727  dl_total:  48.10488963127136  tr_total:  49.528679847717285  loss:  1248.0914306640625\n",
            "Data loader Batch:  8 Time:  6.010538101196289  dl_total:  53.95316481590271  tr_total:  55.539217948913574  loss:  1281.3922119140625\n",
            "Data loader Batch:  9 Time:  6.200701713562012  dl_total:  60.004546880722046  tr_total:  61.739919662475586  loss:  1277.41845703125\n",
            "Data loader Batch:  10 Time:  6.284758806228638  dl_total:  66.13375163078308  tr_total:  68.02467846870422  loss:  1190.827392578125\n",
            "Data loader Batch:  11 Time:  6.124060392379761  dl_total:  72.10679984092712  tr_total:  74.14873886108398  loss:  1306.5711669921875\n",
            "Data loader Batch:  12 Time:  6.152785778045654  dl_total:  78.10267472267151  tr_total:  80.30152463912964  loss:  1275.6988525390625\n",
            "Data loader Batch:  13 Time:  6.1026105880737305  dl_total:  84.05219507217407  tr_total:  86.40413522720337  loss:  1282.634521484375\n",
            "Data loader Batch:  14 Time:  6.177139520645142  dl_total:  90.07255911827087  tr_total:  92.58127474784851  loss:  1333.56640625\n",
            "Data loader Batch:  15 Time:  6.122982978820801  dl_total:  96.0427918434143  tr_total:  98.70425772666931  loss:  1394.4525146484375\n",
            "Data loader Batch:  16 Time:  6.203436613082886  dl_total:  102.09327268600464  tr_total:  104.9076943397522  loss:  1314.828369140625\n",
            "Data loader Batch:  17 Time:  6.130152702331543  dl_total:  108.07260847091675  tr_total:  111.03784704208374  loss:  1315.08935546875\n",
            "Data loader Batch:  18 Time:  6.23950457572937  dl_total:  114.15504145622253  tr_total:  117.27735161781311  loss:  1373.2314453125\n",
            "Data loader Batch:  19 Time:  6.133790969848633  dl_total:  120.13878011703491  tr_total:  123.41114258766174  loss:  1263.185302734375\n",
            "Data loader Batch:  20 Time:  6.040018558502197  dl_total:  126.03062462806702  tr_total:  129.45116114616394  loss:  1234.4368896484375\n",
            "Data loader Batch:  21 Time:  6.154531478881836  dl_total:  132.03542685508728  tr_total:  135.60569262504578  loss:  1277.201171875\n",
            "Data loader Batch:  22 Time:  6.257684707641602  dl_total:  138.13954043388367  tr_total:  141.86337733268738  loss:  1291.1748046875\n",
            "Data loader Batch:  23 Time:  6.148698091506958  dl_total:  144.13129568099976  tr_total:  148.01207542419434  loss:  1309.4305419921875\n",
            "Data loader Batch:  24 Time:  6.198626518249512  dl_total:  150.17701244354248  tr_total:  154.21070194244385  loss:  1279.6470947265625\n",
            "Data loader Batch:  25 Time:  6.1411449909210205  dl_total:  156.15360021591187  tr_total:  160.35184693336487  loss:  1251.29296875\n",
            "Data loader Batch:  26 Time:  6.219298601150513  dl_total:  162.21340608596802  tr_total:  166.57114553451538  loss:  1244.423095703125\n",
            "Data loader Batch:  27 Time:  6.126363277435303  dl_total:  168.1741509437561  tr_total:  172.69750881195068  loss:  1323.915283203125\n",
            "Data loader Batch:  28 Time:  6.268710613250732  dl_total:  174.26546621322632  tr_total:  178.96621942520142  loss:  1266.498779296875\n",
            "Data loader Batch:  29 Time:  6.100898504257202  dl_total:  180.1936228275299  tr_total:  185.06711792945862  loss:  1318.501953125\n",
            "Data loader Batch:  30 Time:  6.2093825340271  dl_total:  186.2192165851593  tr_total:  191.27650046348572  loss:  1105.3330078125\n",
            "Data loader Batch:  31 Time:  5.941643476486206  dl_total:  191.9909040927887  tr_total:  197.21814393997192  loss:  1256.77392578125\n",
            "Data loader Batch:  32 Time:  6.179208755493164  dl_total:  198.01626634597778  tr_total:  203.3973526954651  loss:  1276.9354248046875\n",
            "Data loader Batch:  33 Time:  6.128971338272095  dl_total:  203.99262642860413  tr_total:  209.52632403373718  loss:  1315.8154296875\n",
            "Data loader Batch:  34 Time:  6.235253810882568  dl_total:  210.05500602722168  tr_total:  215.76157784461975  loss:  1254.6741943359375\n",
            "Data loader Batch:  35 Time:  6.189147710800171  dl_total:  216.0957911014557  tr_total:  221.95072555541992  loss:  1247.6873779296875\n",
            "Data loader Batch:  36 Time:  6.222661733627319  dl_total:  222.16757917404175  tr_total:  228.17338728904724  loss:  1312.126953125\n",
            "Data loader Batch:  37 Time:  6.179236650466919  dl_total:  228.20137786865234  tr_total:  234.35262393951416  loss:  1172.580322265625\n",
            "Data loader Batch:  38 Time:  6.244711399078369  dl_total:  234.28949737548828  tr_total:  240.59733533859253  loss:  1169.175537109375\n",
            "Data loader Batch:  39 Time:  6.20089864730835  dl_total:  240.3129231929779  tr_total:  246.79823398590088  loss:  1153.6146240234375\n",
            "Data loader Batch:  40 Time:  6.177654266357422  dl_total:  246.33269500732422  tr_total:  252.9758882522583  loss:  1136.974365234375\n",
            "Data loader Batch:  41 Time:  6.257079362869263  dl_total:  252.40979743003845  tr_total:  259.23296761512756  loss:  1191.81298828125\n",
            "Data loader Batch:  42 Time:  5.990292310714722  dl_total:  258.2485899925232  tr_total:  265.2232599258423  loss:  1171.9522705078125\n",
            "Data loader Batch:  43 Time:  6.187454462051392  dl_total:  264.2470715045929  tr_total:  271.4107143878937  loss:  1220.001708984375\n",
            "Data loader Batch:  44 Time:  6.17354679107666  dl_total:  270.2649517059326  tr_total:  277.58426117897034  loss:  1125.364013671875\n",
            "Data loader Batch:  45 Time:  6.304348945617676  dl_total:  276.3970170021057  tr_total:  283.888610124588  loss:  1147.6199951171875\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'traceback' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-c85a0b7d6836>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             train_loss, dl_tt, tr_tt = train(model, train_dataloader, sampler,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                criterion, optimizer, args)\n",
            "\u001b[0;32m<ipython-input-112-c7f5c6e6fd89>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, sampler, criterion, optimizer, args)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtr_tt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_pair_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_pair_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# data_loader_etime = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dgl/dataloading/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_threaded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dgl/dataloading/dataloader.py\u001b[0m in \u001b[0;36m_next_threaded\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m             batch, feats, stream_event, exception = self.queue.get(\n\u001b[0m\u001b[1;32m    696\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefetcher_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-c85a0b7d6836>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0merror_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Training Interreputed!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'traceback' is not defined"
          ]
        }
      ],
      "source": [
        "sampler = temporal_edge_sampler\n",
        "print(\"At the beginning\")\n",
        "i = -1\n",
        "# nn_test_ap, nn_test_auc = test_val(\n",
        "#                 model, test_new_node_dataloader, sampler, criterion, args)\n",
        "# test_ap, test_auc = test_val(\n",
        "#                 model, test_dataloader, sampler, criterion, args)\n",
        "# print(\"Epoch: {}; Test AP: {:.3f} AUC: {:.3f}\\n\".format(i, test_ap, test_auc))\n",
        "# print(\"Epoch: {}; Test New Node AP: {:.3f} AUC: {:.3f}\\n\".format(\n",
        "#                 i, nn_test_ap, nn_test_auc))\n",
        "\n",
        "try:\n",
        "        for i in range(args.epochs):\n",
        "            train_loss, dl_tt, tr_tt = train(model, train_dataloader, sampler,\n",
        "                               criterion, optimizer, args)\n",
        "            # print(\"total\")\n",
        "            val_ap, val_auc = test_val(\n",
        "                model, valid_dataloader, sampler, criterion, args)\n",
        "            memory_checkpoint = model.store_memory()\n",
        "            # print(memory_checkpoint)\n",
        "            # if args.fast_mode:\n",
        "            #     new_node_sampler.sync(sampler)\n",
        "            test_ap, test_auc = test_val(\n",
        "                model, test_dataloader, sampler, criterion, args)\n",
        "            model.restore_memory(memory_checkpoint)\n",
        "            # print(\"after restoring: \", model.memory.memory)\n",
        "            # if args.fast_mode:\n",
        "            #     sample_nn = new_node_sampler\n",
        "            # else:\n",
        "            sample_nn = sampler\n",
        "            nn_test_ap, nn_test_auc = test_val(\n",
        "                model, test_new_node_dataloader, sample_nn, criterion, args)\n",
        "            log_content = []\n",
        "            log_content.append(\"Epoch: {}; Training Loss: {} | Validation AP: {:.3f} AUC: {:.3f}\\n\".format(\n",
        "                i, train_loss, val_ap, val_auc))\n",
        "            log_content.append(\n",
        "                \"Epoch: {}; Test AP: {:.3f} AUC: {:.3f}\\n\".format(i, test_ap, test_auc))\n",
        "            log_content.append(\"Epoch: {}; Test New Node AP: {:.3f} AUC: {:.3f}\\n\".format(\n",
        "                i, nn_test_ap, nn_test_auc))\n",
        "            log_content.append(\"total time: dataloading: {} and overall: {}\\n\".format(dl_tt, tr_tt))\n",
        "            f.writelines(log_content)\n",
        "            # print(\"before reset: \", model.memory.memory)\n",
        "            model.reset_memory()\n",
        "            # print(\"after reset: \", model.memory.memory)\n",
        "            if i < args.epochs-1 and args.fast_mode:\n",
        "                sampler.reset()\n",
        "            print(log_content[0], log_content[1], log_content[2])\n",
        "except KeyboardInterrupt:\n",
        "        traceback.print_exc()\n",
        "        error_content = \"Training Interreputed!\"\n",
        "        f.writelines(error_content)\n",
        "        f.close()\n",
        "print(\"========Training is Done========\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p, q = graph_no_new_node.edges()\n",
        "print(p[:8])\n",
        "print(q[:8])\n",
        "print(graph_no_new_node.ndata['_ID'])\n"
      ],
      "metadata": {
        "id": "JDrPpKOdGc9v"
      },
      "id": "JDrPpKOdGc9v",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:.conda-tgnnv2]",
      "language": "python",
      "name": "conda-env-.conda-tgnnv2-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}